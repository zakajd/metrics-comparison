{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T09:14:26.451984Z",
     "start_time": "2020-06-12T09:14:26.414344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from photosynthesis_metrics import vif_p\n",
    "import torch\n",
    "x = torch.rand(1, 3, 256, 256).cuda()\n",
    "y = torch.rand(1, 3, 256, 256).cuda()\n",
    "# torch.isnan(vif_p(x, x))\n",
    "score = vif_p(x, x)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T16:34:19.630158Z",
     "start_time": "2020-06-12T16:34:19.615355Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-d1dd02fc8d22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9345\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# measure.dim()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# len(measure) == measure.size(0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "# measure = torch.tensor([0.9345, 1])\n",
    "measure = torch.tensor(0.9345)\n",
    "# measure.dim()\n",
    "# measure.requires_grad()\n",
    "# len(measure) == measure.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:03:09.786046Z",
     "start_time": "2020-06-12T10:03:09.781125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  2.0237,  ..., -0.6767, -1.2737,  0.0000],\n",
       "          [ 0.0000, -0.5694, -0.9300,  ...,  0.0000,  0.0000, -0.6459],\n",
       "          [-0.3101,  1.1049,  0.0000,  ...,  0.0000,  0.0000,  1.1628],\n",
       "          ...,\n",
       "          [ 0.9193,  0.0000,  0.0000,  ..., -2.0906,  0.0000,  0.0000],\n",
       "          [ 0.4722,  0.0250,  0.0000,  ..., -0.4501, -0.9140,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.5370,  ..., -0.8256, -0.3342,  0.4103]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000, -0.2095,  0.1179,  ...,  0.0000,  0.0000, -0.8552],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.8735, -0.5826, -0.0311],\n",
       "          [-0.1037,  0.9752,  0.2611,  ...,  0.0000,  0.2897, -0.3917],\n",
       "          ...,\n",
       "          [ 0.0000, -0.3465,  0.0000,  ...,  0.0000, -0.3884,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ..., -0.0795,  0.0000,  0.0000],\n",
       "          [-1.0346,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.4758]]],\n",
       "\n",
       "\n",
       "        [[[-0.0683,  0.0000,  0.0000,  ...,  0.8636,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.2265,  ...,  0.0000,  0.9316, -0.2529],\n",
       "          [ 0.0000,  0.1836,  0.0103,  ...,  0.2414,  0.2110,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.1556,  0.0422,  ..., -1.0880,  0.0000, -1.0327],\n",
       "          [-0.3275,  0.7142, -1.0672,  ..., -0.1337,  0.0000,  0.0000],\n",
       "          [ 0.0000, -0.2041,  0.5039,  ..., -0.2125,  0.0000, -0.5838]]],\n",
       "\n",
       "\n",
       "        [[[-0.0620, -1.5680,  2.3670,  ...,  0.6163, -0.3723, -0.6250],\n",
       "          [ 0.0000, -0.6374, -0.1296,  ...,  1.1375,  0.0000, -0.1052],\n",
       "          [ 0.1605,  0.0000, -0.0851,  ...,  0.8268,  1.2062,  0.5630],\n",
       "          ...,\n",
       "          [-0.1000,  0.0000,  0.0000,  ...,  0.0000, -0.4442,  0.0000],\n",
       "          [ 0.7106,  0.0000,  0.0000,  ...,  1.1332,  0.0040,  0.0000],\n",
       "          [ 0.0000, -0.5009,  0.0000,  ...,  0.0000,  0.0000,  0.6846]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigma_target_sq = torch.randn(4, 1, 64, 64)\n",
    "# g = torch.randn(4, 1, 64, 64)\n",
    "g *= sigma_target_sq >= EPS\n",
    "# g[sigma_target_sq < EPS] = 0\n",
    "g\n",
    "# g.where(sigma_target_sq >= 0, torch.zeros_like(g)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:43:25.009663Z",
     "start_time": "2020-06-12T10:43:23.499479Z"
    }
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import sys\n",
    "import random \n",
    "import functools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as albu_pt\n",
    "import photosynthesis_metrics as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-12T10:43:26.274437Z",
     "start_time": "2020-06-12T10:43:26.023589Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 2020 \n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.0\n",
      "torch 1.5.0\n",
      "albumentations 0.4.5\n",
      "photosynthesis_metrics 0.4.0\n",
      "\n",
      "compiler   : GCC 8.4.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-99-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "# Fix to be able to import python modules inside a notebook\n",
    "os.chdir('..')\n",
    "\n",
    "# Useful extensions\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,torch,albumentations,photosynthesis_metrics\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Nice plot formating\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ð¡ompute metric predictions for TID2013 \n",
    "\n",
    "1. Prediction in form 1, 121, 241, ...\n",
    "2. want to be 1, 2, 3, 4, 5, 6, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:25:20.333609Z",
     "start_time": "2020-06-11T15:25:20.295580Z"
    }
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from src.modules.models import Regression\n",
    "from src.data.datasets import TID2013\n",
    "from src.data.utils import crop_patches\n",
    "from src.modules.losses import ContentLoss, StyleLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:25:20.627353Z",
     "start_time": "2020-06-11T15:25:20.624171Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PSNR(_Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        mse = torch.mean((prediction - target) ** 2, dim=[1,2,3])\n",
    "        psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "        return psnr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:25:22.733724Z",
     "start_time": "2020-06-11T15:25:22.729123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Init Full Reference metrics\n",
    "psnr = PSNR()\n",
    "ssim = functools.partial(pm.ssim, data_range=1.0, size_average=False, full=False)\n",
    "ms_ssim = functools.partial(pm.multi_scale_ssim, data_range=1.0, size_average=False)\n",
    "vif_p = functools.partial(pm.vif_p, data_range=1.0)\n",
    "gmsd = pm.GMSDLoss(reduction='none', data_range=1.0)\n",
    "ms_gmsd = pm.MultiScaleGMSDLoss(reduction='none', data_range=1.0)\n",
    "\n",
    "# style = StyleLoss(reduction='none')\n",
    "# content = ContentLoss(reduction='none')\n",
    "\n",
    "# Init No Reference metrics\n",
    "brisque = pm.BRISQUELoss(reduction='none')\n",
    "\n",
    "# Init Distribution based metrics\n",
    "fid = pm.FID()\n",
    "kid = pm.KID()\n",
    "# gs = pm.GS()\n",
    "isc = pm.IS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:25:38.208260Z",
     "start_time": "2020-06-11T15:25:30.457905Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08028178025345c9b7ab5b362a210af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Init dataset\n",
    "transform = albu.Compose([\n",
    "    albu.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.], ), # to [0, 1]\n",
    "    albu_pt.ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = TID2013(transform=transform)\n",
    "\n",
    "# Iterate over dataset\n",
    "metric_scores = defaultdict(list)\n",
    "for idx in tqdm(range(len(dataset))):\n",
    "    # Load images\n",
    "    distorted_images, reference_images, _ = dataset[idx]\n",
    "    distorted_images, reference_images = distorted_images.to(\"cuda\"), reference_images.to(\"cuda\")\n",
    "    \n",
    "    # Compute metrics\n",
    "    metric_scores['psnr'].append(psnr(distorted_images, reference_images))\n",
    "    metric_scores['ssim'].append(ssim(distorted_images, reference_images))\n",
    "    metric_scores['ms_ssim'].append(ms_ssim(distorted_images, reference_images))\n",
    "    metric_scores['vif_p'].append(vif_p(distorted_images, reference_images))\n",
    "    metric_scores['gmsd'].append(gmsd(distorted_images, reference_images))\n",
    "    metric_scores['ms_gmsd'].append(ms_gmsd(distorted_images, reference_images))\n",
    "    metric_scores['brisque'].append(brisque(distorted_images))\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         metric_scores['style'].append(style(distorted_images, reference_images))\n",
    "#         metric_scores['content'].append(content(distorted_images, reference_images))\n",
    "    break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:27:18.430214Z",
     "start_time": "2020-06-11T15:27:14.600221Z"
    }
   },
   "outputs": [],
   "source": [
    "from photosynthesis_metrics.feature_extractors.fid_inception import InceptionV3\n",
    "distorted_patches = crop_patches(distorted_images)\n",
    "feature_extractor = InceptionV3(use_fid_inception=False, normalize_input=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:30:10.703839Z",
     "start_time": "2020-06-11T15:30:10.701520Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:30:57.345639Z",
     "start_time": "2020-06-11T15:30:57.342639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([125, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# distorted_patches.shape\n",
    "loader = distorted_patches.view(-1, BATCH_SIZE, *distorted_patches.shape[-3:])\n",
    "for distorted_patche in loader:\n",
    "    print(distorted_patche.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T15:27:30.082571Z",
     "start_time": "2020-06-11T15:27:29.934530Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.12 GiB (GPU 0; 31.75 GiB total capacity; 8.54 GiB already allocated; 2.90 GiB free; 8.55 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-28790ad6e58d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistorted_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/photosynthesis_metrics/feature_extractors/fid_inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Scale from range (0, 1) to range (-1, 1).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.12 GiB (GPU 0; 31.75 GiB total capacity; 8.54 GiB already allocated; 2.90 GiB free; 8.55 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "features = feature_extractor(distorted_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-init dataset with different normalization\n",
    "transform = albu.Compose([\n",
    "    albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\n",
    "    albu_pt.ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = TID2013(transform=transform)\n",
    "\n",
    "# Iterate over dataset\n",
    "for idx in tqdm(range(len(dataset))):\n",
    "    # Load images\n",
    "    distorted_images, reference_images, _ = dataset[idx]\n",
    "    \n",
    "    # Create patches \n",
    "    distorted_patches = crop_patches(distorted_images)\n",
    "    reference_patches = crop_patches(reference_images)\n",
    "    \n",
    "    # Compute features\n",
    "    \n",
    "    fid_score = \n",
    "    metric_scores['fid'].append(fid(distorted_images, reference_images))\n",
    "    predicted_scores = metric(distorted_images, reference_images)\n",
    "    metric_scores.append(predicted_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T14:43:35.921829Z",
     "start_time": "2020-06-11T14:41:00.363452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000])\n",
      "torch.Size([3000])\n",
      "torch.Size([3000])\n",
      "torch.Size([3000])\n",
      "torch.Size([3000])\n",
      "torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "for key in metric_scores.keys():\n",
    "    value = metric_scores[key]\n",
    "    # Reduce\n",
    "    scores = torch.stack(value).t().flatten()\n",
    "    print(key, scores.shape)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(f\"data/interim/{key}.txt\", \"x\") as file:\n",
    "        file.write('\\n'.join(str(score) for score in scores.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result to file\n",
    "with open(\"data/interim/ssim.txt\", \"x\") as file:\n",
    "    file.write('\\n'.join(str(score) for score in metric_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"ssim\": pm.ssim,\n",
    "    \"ms-ssim\": pm.multi_scale_ssim,\n",
    "    \"vif\": pm.vif_p,\n",
    "    \"gmsd\": pm.GMSDLoss,\n",
    "    \"ms-gmsd\": pm.MultiScaleGMSDLoss,\n",
    "    \"msid\": pm.MSID,\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:01:40.726099Z",
     "start_time": "2020-06-08T19:01:40.721084Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:05:57.430227Z",
     "start_time": "2020-06-08T19:05:57.425493Z"
    }
   },
   "outputs": [],
   "source": [
    "r = [torch.rand(4), torch.rand(4), torch.rand(4)]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:08:15.479284Z",
     "start_time": "2020-06-08T19:08:15.470561Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:09:26.716089Z",
     "start_time": "2020-06-08T19:09:26.711457Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.stack(r).t().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:11:47.797251Z",
     "start_time": "2020-06-08T19:10:59.078116Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T13:32:29.209149Z",
     "start_time": "2020-06-11T13:32:29.198747Z"
    }
   },
   "outputs": [],
   "source": [
    "# MOS scores ordered by image name\n",
    "file_path = \"data/raw/tid2013/mos.txt\"\n",
    "# Read file mith MOS\n",
    "with open(file_path) as f:\n",
    "    mos_scores = f.readlines()\n",
    "mos_scores = [float(score) for score in mos_scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:14:54.717747Z",
     "start_time": "2020-06-08T19:14:54.707926Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T19:14:55.947099Z",
     "start_time": "2020-06-08T19:14:55.936663Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "x = np.arange(10, 20)\n",
    "y = np.array([2, 1, 4, 5, 8, 12, 18, 25, 96, 48])\n",
    "x_t = torch.tensor(x, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "r = np.corrcoef(x, y)[0, 1]\n",
    "r_t = pearson_correlation(x_t, y_t)\n",
    "print(r, r_t)\n",
    "\n",
    "print(scipy.stats.pearsonr(x, y)[0])    # Pearson's r\n",
    "\n",
    "print(scipy.stats.spearmanr(x, y)[0])   # Spearman's rho\n",
    "\n",
    "print(scipy.stats.kendalltau(x, y)[0])  # Kendall's tau\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-08T19:14:56.665Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load MOS score\n",
    "# 2. Load score for FSIMc\n",
    "# 3. Fit regression using MSE loss\n",
    "# 4. Fit regression using PLCC loss\n",
    "# 5. Compute PLCC, SRCC in both cases\n",
    "# 6. Compare with values from the paper\n",
    "\n",
    "learningRate = 0.01\n",
    "epochs = 10000\n",
    "    \n",
    "metric_names = [\"PSNRHVSM\", \"VIFP\", \"MSSIM\", \n",
    "#                 \"WSNR\", \"PSNRHMA\", \"FSIM\", \"SSIM\", \"PSNRc\", \"PSNR\", \"FSIMc\", \n",
    "#                \"PSNRHVS\", \"PSNRHA\", \"VSNR\", \"NQM\"\n",
    "               ]\n",
    "\n",
    "\n",
    "for metric in metric_names:\n",
    "    done = False\n",
    "    metric_path = \"data/raw/tid2013/metrics_values/\" + metric + \".txt\"\n",
    "    # Read metric score\n",
    "    with open(metric_path) as f:\n",
    "        metric_scores = f.readlines()\n",
    "\n",
    "    metric_scores = [float(score) for score in metric_scores]\n",
    "\n",
    "    model = Regression()\n",
    "#     criterion = torch.nn.MSELoss() \n",
    "    criterion = functools.partial(pearson_correlation, invert=True)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learningRate)\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=100, cooldown=50)\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=learningRate, max_lr=0.05)\n",
    "\n",
    "    prediction = torch.tensor(metric_scores)\n",
    "    target = torch.tensor(mos_scores)\n",
    "\n",
    "    # Fit regression\n",
    "    for epoch in range(epochs + 1):\n",
    "        if done:\n",
    "            continue\n",
    "        # Clear gradient \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get output from the model, given the inputs\n",
    "        outputs = model(prediction)\n",
    "\n",
    "        # Get loss for the predicted output\n",
    "        loss = criterion(outputs, target)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"NaN in loss!\")\n",
    "            done = True\n",
    "            continue\n",
    "            \n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        if epoch % 5000 == 0:\n",
    "            print(f'{metric} ({epoch}): {loss.item()}')\n",
    "    \n",
    "    if done:\n",
    "        continue\n",
    "    # Compute metrics:\n",
    "    x = outputs.detach().numpy()\n",
    "    y = mos_scores\n",
    "    print(f\"{metric}: PLCC {pearsonr(x, y)[0]:0.3f}, SRCC {spearmanr(x, y)[0]:0.3f}\", \n",
    "          f\"KRCC {kendalltau(x, y)[0]:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = \"\"\"VIFP: PLCC 0.752 SRCC 0.658 KRCC 0.481\n",
    "MSSIM PLCC 0.831 SRCC 0.787 KRCC 0.608\n",
    "PSNRHMA PLCC 0.799 SRCC 0.813 KRCC 0.632\n",
    "FSIM PLCC 0.854 SRCC 0.801 KRCC 0.630\n",
    "SSIM PLCC 0.684 SRCC 0.637 KRCC 0.464\n",
    "PSNRc PLCC 0.660 SRCC 0.687 KRCC 0.496\n",
    "FSIMc PLCC 0.870 SRCC 0.851 KRCC 0.667\n",
    "PSNRHA PLCC 0.796 SRCC 0.819 KRCC 0.643\"\"\"\n",
    "result = {}\n",
    "plcc = []\n",
    "srcc = []\n",
    "krcc = []\n",
    "for line in lines.split(\"\\n\"):\n",
    "    split = line.split(\" \")\n",
    "    plcc.append((split[0], float(split[2])))\n",
    "    srcc.append((split[0], float(split[4])))\n",
    "    krcc.append((split[0], float(split[6])))\n",
    "    \n",
    "    result[split[0]] = (float(split[2]), float(split[4]), float(split[6]))\n",
    "    \n",
    "result\n",
    "idx = 1\n",
    "print(\"ÐÐ¾ÑÑÐ¸ÑÐ°Ð½Ð½ÑÐµ Ð¼Ð½Ð¾Ð¹\")\n",
    "for pair in sorted(plcc, key = lambda x: x[1], reverse=True):\n",
    "    print(idx, pair[0], pair[1])\n",
    "    idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ÐÐ°Ð½Ð½ÑÐµ Ð¸Ð· ÑÑÐ°ÑÑÐ¸\n",
    "1  FSIMc   0.851        \n",
    " 2  PSNR-HA 0.819\n",
    " 3  PSNR-HMA 0.813        \n",
    " 4  FSIM 0.801     \n",
    " 5  MSSIM   0.787         \n",
    " 6  PSNRc   0.687        \n",
    " 7 SSIM 0.637                                        \n",
    " 8 VIFP 0.608                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph: Ox - metric value, Oy - MOS. Blue cross - image in dataset. Black line - fitted logregression\n",
    "plt.plot(metric_scores, mos_scores, \"+\")\n",
    "x = np.arange(0.0, 1.0, 0.005)\n",
    "y = model(torch.tensor(x))\n",
    "plt.plot(x, y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FSIMc\n",
    "MSE\n",
    "epoch 9000, loss 0.4084590971469879\n",
    "tensor(0.8559) \n",
    "\n",
    "PLCC\n",
    "epoch 10000, loss 0.14436256885528564\n",
    "tensor(0.8556)\n",
    "\n",
    "VIFP\n",
    "PLCC\n",
    "tensor(0.7530)\n",
    "\n",
    "MSE\n",
    "epoch1000 loss 0.6762036681175232\n",
    "tensor(0.7519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict PM lib scores, fit regression, get ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIF (debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T13:47:23.482990Z",
     "start_time": "2020-06-15T13:47:23.404790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0007, grad_fn=<AddBackward0>) None\n"
     ]
    }
   ],
   "source": [
    "r\"\"\"Implemetation of Visual Information Fidelity metric\n",
    "Code is based on MATLAB version for computations in pixel domain\n",
    "https://live.ece.utexas.edu/research/Quality/VIF.htm\n",
    "\n",
    "References:\n",
    "    https://ieeexplore.ieee.org/abstract/document/1576816/\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "import torch.nn.functional as F\n",
    "from typing import Union\n",
    "\n",
    "from photosynthesis_metrics.utils import _adjust_dimensions, _validate_input\n",
    "\n",
    "\n",
    "def _gaussian_kernel2d(kernel_size: int = 5, sigma: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Returns 2D Gaussian kernel N(0,`sigma`)\n",
    "    Args:\n",
    "        kernel_size: Size\n",
    "        sigma: Sigma\n",
    "    Returns:\n",
    "        gaussian_kernel: 2D kernel with shape (kernel_size x kernel_size)\n",
    "        \n",
    "    \"\"\"\n",
    "    x = torch.arange(- (kernel_size // 2), kernel_size // 2 + 1).resize(1, kernel_size)\n",
    "    y = torch.arange(- (kernel_size // 2), kernel_size // 2 + 1).resize(kernel_size, 1)\n",
    "    kernel = torch.exp(-(x * x + y * y) / (2.0 * sigma ** 2))\n",
    "    # Normalize\n",
    "    kernel = kernel / torch.sum(kernel)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    \n",
    "    x = torch.randn((4, 3, 128, 128), requires_grad=True)\n",
    "    y = torch.randn((4, 3, 128, 128), requires_grad=True)\n",
    "\n",
    "    prediction = torch.randn((4, 3, 128, 128), requires_grad=True)\n",
    "    target = torch.randn((4, 3, 128, 128), requires_grad=True)\n",
    "#     prediction = x.clone() + 2\n",
    "#     target = y.clone() + 2\n",
    "    \n",
    "    sigma_n_sq: float = 2.0\n",
    "    data_range: int = 1.0\n",
    "    reduction: str = 'mean'\n",
    "\n",
    "    _validate_input((prediction, target), allow_5d=False)\n",
    "    if data_range == 255:\n",
    "        prediction = prediction / 255.\n",
    "        target = target / 255.\n",
    "\n",
    "    # Convert RGB image to YCbCr and take luminance: Y = 0.299 R + 0.587 G + 0.114 B\n",
    "    num_channels = prediction.size(1)\n",
    "#     if num_channels == 3:\n",
    "#         Y_weights = torch.tensor([[0.299, 0.587, 0.114]]).t().to(prediction)\n",
    "#         print(prediction.shape, target.shape, Y_weights.shape)\n",
    "#         prediction = torch.matmul(prediction.permute(0, 2, 3, 1), Y_weights).permute(0, 3, 1, 2)\n",
    "#         target = torch.matmul(target.permute(0, 2, 3, 1), Y_weights).permute(0, 3, 1, 2)\n",
    "#         print(prediction.shape, target.shape)\n",
    "\n",
    "    if num_channels == 3:\n",
    "        prediction = 0.299 * prediction[:, 0, :, :].clone() + 0.587 * prediction[:, 1, :, :].clone() + 0.114 * prediction[:, 2, :, :].clone()\n",
    "        target = 0.299 * target[:, 0, :, :].clone() + 0.587 * target[:, 1, :, :].clone() + 0.114 * target[:, 2, :, :].clone()\n",
    "\n",
    "        # Add channel dimension\n",
    "        prediction = prediction[:, None, :, :].clone()\n",
    "        target = target[:, None, :, :].clone()\n",
    "\n",
    "\n",
    "    # Constant for numerical stability\n",
    "    EPS = 1e-8\n",
    "    \n",
    "    s = prediction.mean() + target.mean()\n",
    "    s.backward()\n",
    "    print(s, x.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T13:43:33.004194Z",
     "start_time": "2020-06-15T13:43:32.927935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0028, grad_fn=<AddBackward0>) None\n"
     ]
    }
   ],
   "source": [
    "#     # Progressively downsample images and compute VIF on different scales\n",
    "#     prediction_vif, target_vif = 0, 0\n",
    "#     for scale in range(1, 5):\n",
    "#         kernel_size = 2 ** (5 - scale) + 1\n",
    "#         kernel = _gaussian_kernel2d(kernel_size, sigma=kernel_size / 5)\n",
    "#         kernel = kernel.view(1, 1, kernel_size, kernel_size).to(prediction)\n",
    "\n",
    "#         if scale > 1:\n",
    "#             # Convolve and downsample\n",
    "#             prediction = F.conv2d(prediction, kernel)[:, :, ::2, ::2].clone()  # valid padding\n",
    "#             target = F.conv2d(target, kernel)[:, :, ::2, ::2].clone()  # valid padding\n",
    "\n",
    "#         mu_trgt, mu_pred = F.conv2d(target, kernel), F.conv2d(prediction, kernel)  # valid padding\n",
    "#         mu_trgt_sq, mu_pred_sq, mu_trgt_pred = mu_trgt * mu_trgt, mu_pred * mu_pred, mu_trgt * mu_pred\n",
    "\n",
    "#         sigma_trgt_sq = F.conv2d(target ** 2, kernel) - mu_trgt_sq\n",
    "#         sigma_pred_sq = F.conv2d(prediction ** 2, kernel) - mu_pred_sq\n",
    "#         sigma_trgt_pred = F.conv2d(target * prediction, kernel) - mu_trgt_pred\n",
    "        \n",
    "#         # Zero small negative values\n",
    "#         torch.relu_(sigma_trgt_sq)\n",
    "#         torch.relu_(sigma_pred_sq)\n",
    "\n",
    "#         g = sigma_trgt_pred / (sigma_trgt_sq + EPS)\n",
    "#         sigma_v_sq = sigma_pred_sq - g * sigma_trgt_pred\n",
    "\n",
    "# #         g[sigma_trgt_sq < EPS] = 0\n",
    "# #         sigma_v_sq[sigma_trgt_sq < EPS] = sigma_pred_sq[sigma_trgt_sq < EPS]\n",
    "# #         sigma_trgt_sq[sigma_trgt_sq < EPS] = 0\n",
    "\n",
    "# #         g[sigma_pred_sq < EPS] = 0\n",
    "# #         sigma_v_sq[sigma_pred_sq < EPS] = 0\n",
    "\n",
    "# #         sigma_v_sq[g < 0] = sigma_pred_sq[g < 0]\n",
    "#         g = torch.relu(g)\n",
    "#         sigma_v_sq[sigma_v_sq <= EPS] = EPS\n",
    "    \n",
    "    \n",
    "        mask_sigma_trgt_sq = sigma_trgt_sq < EPS\n",
    "        g[mask_sigma_trgt_sq] = 0\n",
    "        sigma_v_sq[mask_sigma_trgt_sq] = sigma_pred_sq[mask_sigma_trgt_sq]\n",
    "        sigma_trgt_sq[mask_sigma_trgt_sq] = 0\n",
    "\n",
    "        mask_sigma_pred_sq = sigma_pred_sq < EPS\n",
    "        g[mask_sigma_pred_sq] = 0\n",
    "        sigma_v_sq[mask_sigma_pred_sq] = 0\n",
    "\n",
    "        mask_g = g < 0\n",
    "        sigma_v_sq[mask_g] = sigma_pred_sq[mask_g]\n",
    "        g = torch.relu(g)\n",
    "        mask_sigma_v_sq = sigma_v_sq <= EPS\n",
    "        sigma_v_sq[mask_sigma_v_sq] = EPS\n",
    "        \n",
    "#         pred_vif_scale = torch.log10(1.0 + (g ** 2.) * sigma_trgt_sq / (sigma_v_sq + sigma_n_sq))\n",
    "#         prediction_vif = prediction_vif + torch.sum(pred_vif_scale, dim=[1, 2, 3])\n",
    "#         target_vif = prediction_vif + torch.sum(torch.log10(1.0 + sigma_trgt_sq / sigma_n_sq), dim=[1, 2, 3])\n",
    "\n",
    "#     vif = (prediction_vif + EPS) / (target_vif + EPS)\n",
    "    \n",
    "#     print(vif)\n",
    "#     vif.mean(dim=0).backward()\n",
    "#     print(vif, target.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T13:33:04.668928Z",
     "start_time": "2020-06-15T13:33:03.361274Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-36e570c41110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mY_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.299\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.587\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.114\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "\n",
    "#     # Reduce if needed\n",
    "#     if reduction == 'mean':\n",
    "#         return vif.mean(dim=0)\n",
    "#     elif reduction == 'sum':\n",
    "#         return vif.sum(dim=0)\n",
    "#     elif reduction != 'none':\n",
    "#         raise ValueError(f'Expected reduction modes are \"mean\"|\"sum\"|\"none\", got {reduction}')\n",
    "#     return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T13:24:43.558119Z",
     "start_time": "2020-06-15T13:24:42.601170Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [4, 1, 3, 3]], which is output 0 of DivBackward0, is at version 3; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-8f3763c55f1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvif_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [4, 1, 3, 3]], which is output 0 of DivBackward0, is at version 3; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "\n",
    "    prediction = torch.randn((4, 3, 64, 64), requires_grad=True)\n",
    "    target = torch.randn((4, 3, 64, 64), requires_grad=True)\n",
    "    score = vif_p(x, y)\n",
    "    score.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDRLoss(nn.Module):\n",
    "    \"\"\"High dynamic range loss.\"\"\"\n",
    "\n",
    "    def __init__(self, eps=0.01):\n",
    "        \"\"\"Initializes loss with numerical stability epsilon.\"\"\"\n",
    "\n",
    "        super(HDRLoss, self).__init__()\n",
    "        self._eps = eps\n",
    "\n",
    "\n",
    "    def forward(self, denoised, target):\n",
    "        \"\"\"Computes loss by unpacking render buffer.\"\"\"\n",
    "\n",
    "        loss = ((denoised - target) ** 2) / (denoised + self._eps) ** 2\n",
    "        return loss.mean(dim=[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of VGG16 loss, originaly used for style transfer and usefull in many other task (including GAN training)\n",
    "It's work in progress, no guarantees that code will work\n",
    "\"\"\"\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "def listify(p):\n",
    "    if p is None:\n",
    "        p = []\n",
    "    elif not isinstance(p, collections.Iterable):\n",
    "        p = [p]\n",
    "    return p\n",
    "\n",
    "\n",
    "class ContentLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Creates content loss for neural style transfer.\n",
    "    Uses pretrained VGG16 model from torchvision by default\n",
    "    layers: list of VGG layers used to evaluate content loss\n",
    "    criterion: str in ['mse', 'mae'], reduction method\n",
    "    reduction: Type of reduction to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers=[\"21\"],\n",
    "        weights=1,\n",
    "        loss=\"mse\",\n",
    "        device=\"cuda\",\n",
    "        reduction=\"mean\",\n",
    "        **args,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = vgg16(pretrained=True, **args)\n",
    "        self.model.eval().to(device)\n",
    "        self.layers = listify(layers)\n",
    "        self.weights = listify(weights)\n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.criterion = nn.MSELoss(reduction=reduction)\n",
    "        elif loss == \"mae\":\n",
    "            self.criterion = nn.L1Loss(reduction=reduction)\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    def forward(self, input, content):\n",
    "        \"\"\"\n",
    "        Measure distance between feature representations of input and content images\n",
    "        \"\"\"\n",
    "        input_features = torch.stack(self.get_features(input))\n",
    "        content_features = torch.stack(self.get_features(content))\n",
    "        loss = self.criterion(input_features, content_features)\n",
    "\n",
    "        # Solve big memory consumption\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract feature maps from the intermediate layers.\n",
    "        \"\"\"\n",
    "        if self.layers is None:\n",
    "            self.layers = [\"21\"]\n",
    "\n",
    "        features = []\n",
    "        for name, module in self.model.features._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layers:\n",
    "                features.append(x)\n",
    "        # print(len(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class StyleLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Class for creating style loss for neural style transfer\n",
    "    model: str in ['vgg16_bn']\n",
    "    reduction: Type of reduction to use\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers=[\"0\", \"5\", \"10\", \"19\", \"28\"],\n",
    "        weights=[0.75, 0.5, 0.2, 0.2, 0.2],\n",
    "        loss=\"mse\",\n",
    "        device=\"cuda\",\n",
    "        reduction=\"mean\",\n",
    "        **args,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = vgg16(pretrained=True, **args)\n",
    "        self.model.eval().to(device)\n",
    "\n",
    "        self.layers = listify(layers)\n",
    "        self.weights = listify(weights)\n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.criterion = nn.MSELoss(reduction=reduction)\n",
    "        elif loss == \"mae\":\n",
    "            self.criterion = nn.L1Loss(reduction=reduction)\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    def forward(self, input, style):\n",
    "        \"\"\"\n",
    "        Measure distance between feature representations of input and content images\n",
    "        \"\"\"\n",
    "        input_features = self.get_features(input)\n",
    "        style_features = self.get_features(style)\n",
    "        # print(style_features[0].size(), len(style_features))\n",
    "\n",
    "        input_gram = [self.gram_matrix(x) for x in input_features]\n",
    "        style_gram = [self.gram_matrix(x) for x in style_features]\n",
    "\n",
    "        loss = [\n",
    "            self.criterion(torch.stack(i_g), torch.stack(s_g)) for i_g, s_g in zip(input_gram, style_gram)\n",
    "        ]\n",
    "        return torch.mean(torch.tensor(loss))\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract feature maps from the intermediate layers.\n",
    "        \"\"\"\n",
    "        if self.layers is None:\n",
    "            self.layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "        features = []\n",
    "        for name, module in self.model.features._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        \"\"\"\n",
    "        Compute Gram matrix for each image in batch\n",
    "        input: Tensor of shape BxCxHxW\n",
    "            B: batch size\n",
    "            C: channels size\n",
    "            H&W: spatial size\n",
    "        \"\"\"\n",
    "\n",
    "        B, C, H, W = input.size()\n",
    "        gram = []\n",
    "        for i in range(B):\n",
    "            x = input[i].view(C, H * W)\n",
    "            gram.append(torch.mm(x, x.t()))\n",
    "        return gram\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
