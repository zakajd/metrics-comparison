{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jamil zakirov\n",
    "jah zak\n",
    "bon\n",
    "jamil.zakir0v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:29:56.064390Z",
     "start_time": "2020-06-08T08:29:56.054211Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random \n",
    "import functools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as albu_pt\n",
    "import photosynthesis_metrics as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:30:20.681380Z",
     "start_time": "2020-06-08T08:30:20.654901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Mon Jun 08 2020 \n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.0\n",
      "torch 1.5.0\n",
      "albumentations 0.4.5\n",
      "photosynthesis_metrics 0.3.0\n",
      "\n",
      "compiler   : GCC 8.4.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-99-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "# Fix to be able to import python modules inside a notebook\n",
    "os.chdir('..')\n",
    "\n",
    "# Useful extensions\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,torch,albumentations,photosynthesis_metrics\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Nice plot formating\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T12:24:03.103357Z",
     "start_time": "2020-04-26T12:24:03.099630Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mnist = torchvision.datasets.MNIST(\"../datasets\", train=True, download=True)\n",
    "# cifar10 = torchvision.datasets.CIFAR10(\"../datasets\", download=True)\n",
    "# cifar100 = torchvision.datasets.CIFAR100(\"../datasets\", download=True)\n",
    "# fashin_mnist = torchvision.datasets.FashionMNIST(\"datasets\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T17:41:48.647750Z",
     "start_time": "2020-06-03T17:41:24.437192Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# !wget -P datasets/ http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !wget -P datasets/ http://vllab.ucmerced.edu/wlai24/LapSRN/results/SR_testing_datasets.zip\n",
    "# !wget -P datasets/ http://www.cs.columbia.edu/CAVE/databases/SLAM_coil-20_coil-100/coil-100/coil-100.zip\n",
    "# !wget -P datasets/ http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip\n",
    "# !wget -P datasets/ http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip\n",
    "# !wget -P datasets/ http://www.ponomarenko.info/tid2013/tid2013.rar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TID2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:07:18.946020Z",
     "start_time": "2020-06-11T12:07:18.929999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'albu_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ad52812453c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malbu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0malbu_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'albu_pt' is not defined"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import TID2013\n",
    "\n",
    "transform = albu.Compose([\n",
    "#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\n",
    "    albu.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.]), # to [0, 1]\n",
    "    albu_pt.ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = TID2013(transform=transform)\n",
    "distorted_images, reference_images, scores = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:50:26.291463Z",
     "start_time": "2020-06-08T08:50:26.122445Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.augmentations import get_aug\n",
    "from src.utils import walk_files\n",
    "from src.datasets import *\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:53:13.529231Z",
     "start_time": "2020-06-08T08:50:31.586592Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c99e75bb6d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repoz/metrics-comparison/src/augmentations.py\u001b[0m in \u001b[0;36mget_aug\u001b[0;34m(aug_type, task, dataset, size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Add the same noise for all channels for single-channel images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMEAN_STD_BY_NAME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"medicaldecathlon\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msinglechannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "TASK = \"denoise\"\n",
    "SIZE = 256\n",
    "\n",
    "dataset_class = {\n",
    "    \"mnist\": MNIST,\n",
    "    \"fashion_mnist\": FashionMNIST, \n",
    "    \"cifar10\": CIFAR10,\n",
    "    \"cifar100\": CIFAR100,\n",
    "    \"tinyimagenet\": TinyImageNet,\n",
    "    \"div2k\": DIV2K,\n",
    "    \"set5\": Set5,\n",
    "    \"set14\": Set14,\n",
    "    \"urba100\": Urban100,\n",
    "    \"manga109\": Manga109,\n",
    "    \"coil100\": COIL100,\n",
    "    \"bsds100\": BSDS100,\n",
    "    \"medicaldecathlon\": MedicalDecathlon\n",
    "}\n",
    "\n",
    "\n",
    "dataset_names = [\n",
    "    \"div2k\",\n",
    "    \"bsds100\",\n",
    "#     \"set5\"\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for dataset_name in dataset_names:\n",
    "    transform = get_aug(aug_type=\"medium\", task=TASK, dataset=datasets, size=SIZE)\n",
    "    datasets.append(dataset_class[dataset_name](train=True, transform=transform))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    input, target = dataset[1]\n",
    "    print(\"Input\", input.shape, input.min(), input.max(), input.mean())\n",
    "    print(\"Target\", target.shape, target.min(), target.max(), target.mean())\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0) * 0.5 + 0.5, )\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:03:51.120720Z",
     "start_time": "2020-05-29T13:03:51.118706Z"
    }
   },
   "outputs": [],
   "source": [
    "# total_mean, total_var = [], []\n",
    "# for i in range(len(div2k)):\n",
    "#     image = div2k[i][1]\n",
    "#     total_mean.append(image.mean(dim=[1,2]))\n",
    "#     total_var.append(image.var(dim=[1,2]))\n",
    "# print(torch.stack(total_mean).mean(dim=0))\n",
    "# print(torch.stack(total_var).mean(dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T15:59:56.557719Z",
     "start_time": "2020-05-29T15:59:55.444859Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:28:43.539258Z",
     "start_time": "2020-05-29T12:28:28.385984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"denoise\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:59:14.735977Z",
     "start_time": "2020-04-26T14:58:51.307570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"deblur\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:00:22.313189Z",
     "start_time": "2020-04-26T15:00:22.300261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(image.min(), image.max(), image.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:02.588910Z",
     "start_time": "2020-04-26T10:46:02.235819Z"
    }
   },
   "outputs": [],
   "source": [
    "# idx = 356\n",
    "print(image[..., 0].min(), image[..., 0].max(), image[...,0].mean())\n",
    "# plt.hist()\n",
    "# print(image.min(), image.max(), image.mean())\n",
    "# grey_image = image[..., 0]\n",
    "augmented = AUG(image=image, mask=image)\n",
    "input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "## Get gaussian\n",
    "# random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n",
    "# gauss = random_state.normal(0, 0.1, input.shape)\n",
    "# input = input + gauss\n",
    "# gauss\n",
    "\n",
    "# print(\"Input\", input.shape, input.min(), input.max())\n",
    "# print(\"Target\", target.shape, target.min(), target.max())\n",
    "\n",
    "# augmented = NORM_TO_TENSOR(image=input, mask=target)\n",
    "# input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "print(\"Input\", input.shape, input.min(), input.max())\n",
    "print(\"Target\", target.shape, target.min(), target.max())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(torch.cat([input[0], target[0]], dim=1)) #.permute(1, 2, 0))\n",
    "# plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0))\n",
    "\n",
    "torch.sum(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:15.660432Z",
     "start_time": "2020-04-26T10:46:15.622703Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.mean(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T10:12:01.132632Z",
     "start_time": "2020-04-25T10:12:01.115628Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow((medicaldecathlon[idx][0][..., 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:31:31.339877Z",
     "start_time": "2020-04-24T21:31:31.334100Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(256, 256)\n",
    "a = a[:,:,np.newaxis].repeat(3, axis=2)\n",
    "# a.repeat(3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:04:44.034846Z",
     "start_time": "2020-04-24T21:04:43.900165Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:00:05.101620Z",
     "start_time": "2020-05-29T16:00:05.098635Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:18:18.394762Z",
     "start_time": "2020-05-29T16:18:10.979470Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = [\"div2k\", \"bsds100\", \"set5\"]\n",
    "transform = get_aug(aug_type=\"val\", task=\"deblur\", size=128)\n",
    "\n",
    "for dataset in datasets:\n",
    "#     train_loader = get_dataloader(dataset, transform, train=True, batch_size=64)\n",
    "    \n",
    "#     for batch in train_loader:\n",
    "#         input, output = batch\n",
    "#         print(input.shape, output.shape)\n",
    "        \n",
    "    \n",
    "    val_loader = get_dataloader(dataset, transform, train=False)\n",
    "    for batch in val_loader:\n",
    "        input, output = batch\n",
    "        print(input.shape, output.shape)\n",
    "    \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:14:07.954107Z",
     "start_time": "2020-05-29T16:14:07.942725Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = BSDS100(\"datasets/BSDS100\", train=True, transform=transform)\n",
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:52:16.029947Z",
     "start_time": "2020-04-14T18:52:16.024488Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "image_metrics = [\"kid\", {}, \"ssim\", {}, ]\n",
    "\n",
    "METRIC_FROM_NAME = {\n",
    "    \"ssim\" : pm.SSIMLoss,\n",
    "    \"ms-ssim\" : pm.MultiScaleSSIMLoss,\n",
    "    \"msid\" : pm.MSID,\n",
    "    \"fid\" : pm.FID,\n",
    "    \"kid\" : pm.KID,\n",
    "#     \"content\" : ContentLoss,\n",
    "#     \"style\" : StyleLoss,\n",
    "    \"tv\" : pm.TVLoss,\n",
    "}\n",
    "\n",
    "\n",
    "# for metric in image_metrics:\n",
    "image_metrics = [METRIC_FROM_NAME[metric](**kwargs) for metric, kwargs in zip(image_metrics[::2], image_metrics[1::2])]\n",
    "# list(zip(image_metrics[::2], image_metrics[1::2]))\n",
    "image_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:29:09.787795Z",
     "start_time": "2020-04-14T19:29:09.783997Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "output = OrderedDict({\n",
    "    'loss': 1,\n",
    "#     'mse': mse,\n",
    "#     'psnr': psnr,\n",
    "#     'ssim': ssim_score,\n",
    "#     # 'val_ms_ssim': ms_ssim_score,\n",
    "#     'input_features': input_features,\n",
    "#     'target_features': target_features\n",
    "})\n",
    "output[\"test\"] = 3\n",
    "output[\"foo\"] = \"bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:08:23.644634Z",
     "start_time": "2020-04-26T15:08:23.508619Z"
    }
   },
   "outputs": [],
   "source": [
    "images = torch.rand((512, 2, 32, 32))\n",
    "target = torch.rand((512, 2, 64, 64))\n",
    "print(f\"Before interpolation: images {images.shape}\")\n",
    "images = F.interpolate(images, size=target.shape[-2:], mode=\"bilinear\")\n",
    "print(f\"After interpolation: images {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import b\n",
    "# \n",
    "iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T20:36:25.754041Z",
     "start_time": "2020-04-14T20:36:25.750728Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [\"foo\", \"bar\"]\n",
    "if \"foo\" in a:\n",
    "    print(a.index(\"foo\"))\n",
    "# a.append(\"loss\")\n",
    "# print(a)\n",
    "# a.pop()\n",
    "# a\n",
    "# for i in \"etc\", a:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:36.482862Z",
     "start_time": "2020-04-14T16:45:34.767973Z"
    }
   },
   "outputs": [],
   "source": [
    "features = make_layers(\n",
    "    [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"], \n",
    "    batch_norm=True\n",
    ")\n",
    "vgg_16 = VGG(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:37.757692Z",
     "start_time": "2020-04-14T16:45:37.751504Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:10.728420Z",
     "start_time": "2020-04-14T16:47:09.880830Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(10, 3, 224, 224)\n",
    "layers = None\n",
    "if layers is None:\n",
    "    layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "features = []\n",
    "for name, module in vgg_16.features._modules.items():\n",
    "    x = module(x)\n",
    "    if name in layers:\n",
    "        features.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:17.479314Z",
     "start_time": "2020-04-14T16:47:17.414244Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:30:14.494629Z",
     "start_time": "2020-04-14T14:30:14.490716Z"
    }
   },
   "outputs": [],
   "source": [
    "# from albumentations import ImageOnlyTransform\n",
    "albu.ImageCompression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:20.318480Z",
     "start_time": "2020-04-14T13:26:20.314695Z"
    }
   },
   "outputs": [],
   "source": [
    "target.permute(2, 0, 1).shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:30.322997Z",
     "start_time": "2020-04-14T13:26:30.167211Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(target.permute(2, 0, 1).transpose(0, 2).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:18:02.700784Z",
     "start_time": "2020-04-14T11:18:02.475697Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TinyImageNetDataset(train=True, transform=None, target_transform=None)\n",
    "input, target = dataset[6]\n",
    "# input.shape\n",
    "# plt.imshow(input / 255.)\n",
    "(input / 255.).min(), (input / 255.).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:09:29.108056Z",
     "start_time": "2020-04-14T11:09:29.104053Z"
    }
   },
   "outputs": [],
   "source": [
    "input.transpose(0, 2).transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:02:42.360140Z",
     "start_time": "2020-04-14T11:02:42.355048Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T10:52:13.348128Z",
     "start_time": "2020-04-09T10:52:13.337949Z"
    }
   },
   "outputs": [],
   "source": [
    "# from src.augmentations import get_aug\n",
    "# from src.datasets import MNIST, CIFAR10, CIFAR100\n",
    "from ..src.datasets\n",
    "# from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:11.714091Z",
     "start_time": "2020-04-08T14:27:11.660050Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type=\"light\", size=32)\n",
    "target_transform = get_aug(aug_type=\"val\", size=32)\n",
    "# target_transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:12.035620Z",
     "start_time": "2020-04-08T14:27:11.981391Z"
    }
   },
   "outputs": [],
   "source": [
    "transform, target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.384001Z",
     "start_time": "2020-04-08T14:27:12.625838Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root='../datasets', train=True, transform=transform, target_transform=target_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# valset = CIFAR10(root='../datasets', train=False, transform=transform, target_transform=target_transform)\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=16,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.589831Z",
     "start_time": "2020-04-08T14:27:13.534077Z"
    }
   },
   "outputs": [],
   "source": [
    "image, target = trainset[0]\n",
    "print(image.shape, target.shape)\n",
    "print(image.max(), image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:14.716283Z",
     "start_time": "2020-04-08T14:27:14.483971Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "input, target = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:16.860006Z",
     "start_time": "2020-04-08T14:27:16.789254Z"
    }
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:04.837084Z",
     "start_time": "2020-04-08T15:09:04.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "model.fc = Identity()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:07.391805Z",
     "start_time": "2020-04-08T15:09:07.348692Z"
    }
   },
   "outputs": [],
   "source": [
    "mock = torch.rand((3, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:08.273261Z",
     "start_time": "2020-04-08T15:09:08.185666Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model(mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:15:02.858962Z",
     "start_time": "2020-04-08T15:15:02.810410Z"
    }
   },
   "outputs": [],
   "source": [
    "all_input_features = [result.detach() for _ in range(4)]\n",
    "print(len(all_input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:16:17.105471Z",
     "start_time": "2020-04-08T15:16:17.056511Z"
    }
   },
   "outputs": [],
   "source": [
    "input_features = torch.cat(all_input_features, dim=0)\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:54:58.027998Z",
     "start_time": "2020-04-08T14:54:57.979447Z"
    }
   },
   "outputs": [],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:29:33.595967Z",
     "start_time": "2020-04-24T17:29:33.564780Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:11:48.608920Z",
     "start_time": "2020-04-24T19:11:48.604678Z"
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"datasets/decathlon/colon.h5\"\n",
    "with h5py.File(datapath, \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "#     data_val = f['imgs_validation'][::10]\n",
    "#     data_test = f['imgs_testing'][::10]\n",
    "#     data = np.concatenate((data_val, data_test))\n",
    "#     print(data.shape)\n",
    "#     print(len(data))\n",
    "    \n",
    "\n",
    "# hf = \n",
    "# data_numpy = np.zeros(hf['imgs_train'].shape, dtype=numpy_type)\n",
    "# # hf['dataset_name'].read_direct(n1)\n",
    "# # hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:17:10.798726Z",
     "start_time": "2020-04-24T19:17:10.390446Z"
    }
   },
   "outputs": [],
   "source": [
    "data = hf['imgs_train'][5000]\n",
    "mask = hf['msks_train'][5000]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data.squeeze(), cmap='gray')\n",
    "# plt.imshow(mask.squeeze(), alpha=0.1)\n",
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:13:31.176828Z",
     "start_time": "2020-04-24T19:13:30.998749Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:43:43.812272Z",
     "start_time": "2020-04-24T17:43:43.677479Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:31:12.884247Z",
     "start_time": "2020-04-24T17:31:12.881134Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:06:57.805822Z",
     "start_time": "2020-05-13T16:06:57.802189Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3                                                                                                                 \n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import photosynthesis_metrics as pm    \n",
    "from photosynthesis_metrics import IS\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:07:03.862241Z",
     "start_time": "2020-05-13T16:07:03.179834Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "cifar10 = CIFAR10(root=\"/home/zakirov/repoz/metrics-comparison/datasets\", train=True, transform=aug)\n",
    "cifar10\n",
    "loader = torch.utils.data.DataLoader(cifar10, batch_size=100, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:16:23.449739Z",
     "start_time": "2020-05-13T15:16:20.585333Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = pm.feature_extractors.fid_inception.InceptionV3(\n",
    "#     resize_input=True, \n",
    "#     normalize_input=True,\n",
    "#     requires_grad=False,\n",
    "#     use_fid_inception=False,\n",
    "# )\n",
    "model = inception_v3(pretrained=True, transform_input=False).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:43:51.665311Z",
     "start_time": "2020-05-19T20:43:51.663129Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T18:37:01.177723Z",
     "start_time": "2020-05-18T18:37:01.119361Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = pm.GS(num_iters=20)\n",
    "f1 = torch.rand(1000, 20)\n",
    "f2 = torch.rand(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T21:22:15.649984Z",
     "start_time": "2020-05-19T21:22:15.644455Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIF (debug and compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:39:19.241373Z",
     "start_time": "2020-05-20T14:39:19.235588Z"
    }
   },
   "source": [
    "A computationally simpler, multi-scale pixel domain implementation whose performance is slightly worse than the Wavelet domain version presented in [2] can also be downloaded here . The pixel domain version also uses a scalar Random Field model for natural images, instead of a vector version in [2]. There are advantages to using the VIF in the pixel domain as well as some disadvantages. The principle advantage is computational simplicity. Secondly, the Wavelet transform used in [2] is a highly overcomplete decomposition that adds a lot of linear correlation between coefficients. While the wavelet decomposition allows scale-space-orientation analysis, this makes the assumptions of conditional independence in [2] weaker. Pixel domain methods avoid this weakness of assumption, but do not allow orientation analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read 2 images used in acticle and compute score for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:56:58.494183Z",
     "start_time": "2020-05-20T13:56:58.093684Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type='light', task='deblur', size=64)\n",
    "\n",
    "loader = get_dataloader(\n",
    "    datasets=['tinyimagenet'],\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "for batch in loader:\n",
    "    break\n",
    "    \n",
    "lr = batch[0][1]\n",
    "hr = batch[1][1]\n",
    "print(lr.min(), lr.max())\n",
    "plt.imshow((lr.permute(1,2,0) + 1) * 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:46:39.197135Z",
     "start_time": "2020-05-20T13:46:39.081810Z"
    }
   },
   "source": [
    "# Geometry score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:46:46.800367Z",
     "start_time": "2020-05-20T13:46:46.744487Z"
    }
   },
   "source": [
    "# Gradient Strength Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peceptual loss, Style loss, LPIPS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of VGG16 loss, originaly used for style transfer and usefull in many other task (including GAN training)\n",
    "It's work in progress, no guarantees that code will work\n",
    "\"\"\"\n",
    "import collections\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss \n",
    "from torchvision.models import vgg16\n",
    "\n",
    "\n",
    "\n",
    "def listify(p):\n",
    "    if p is None:\n",
    "        p = []\n",
    "    elif not isinstance(p, collections.Iterable):\n",
    "        p = [p]\n",
    "    return p\n",
    "\n",
    "class PSNR(_Loss):\n",
    "    def forward(self, prediction, target):\n",
    "        mse = torch.mean((prediction - target) ** 2)\n",
    "        psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
    "        return psnr\n",
    "\n",
    "\n",
    "\n",
    "class ContentLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Creates content loss for neural style transfer.\n",
    "    Uses pretrained VGG16 model from torchvision by default\n",
    "    layers: list of VGG layers used to evaluate content loss\n",
    "    criterion: str in ['mse', 'mae'], reduction method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers=[\"21\"],\n",
    "        weights=1,\n",
    "        loss=\"mse\",\n",
    "        device=\"cuda\",\n",
    "        **args,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = vgg16(pretrained=True, **args)\n",
    "        self.model.eval().to(device)\n",
    "        self.layers = listify(layers)\n",
    "        self.weights = listify(weights)\n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif loss == \"mae\":\n",
    "            self.criterion = nn.L1Loss()\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    def forward(self, input, content):\n",
    "        \"\"\"\n",
    "        Measure distance between feature representations of input and content images\n",
    "        \"\"\"\n",
    "        input_features = torch.stack(self.get_features(input))\n",
    "        content_features = torch.stack(self.get_features(content))\n",
    "        loss = self.criterion(input_features, content_features)\n",
    "\n",
    "        # Solve big memory consumption\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract feature maps from the intermediate layers.\n",
    "        \"\"\"\n",
    "        if self.layers is None:\n",
    "            self.layers = [\"21\"]\n",
    "\n",
    "        features = []\n",
    "        for name, module in self.model.features._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layers:\n",
    "                features.append(x)\n",
    "        # print(len(features))\n",
    "        return features\n",
    "\n",
    "\n",
    "class StyleLoss(_Loss):\n",
    "    \"\"\"\n",
    "    Class for creating style loss for neural style transfer\n",
    "    model: str in ['vgg16_bn']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers=[\"0\", \"5\", \"10\", \"19\", \"28\"],\n",
    "        weights=[0.75, 0.5, 0.2, 0.2, 0.2],\n",
    "        loss=\"mse\",\n",
    "        device=\"cuda\",\n",
    "        **args,\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.model = vgg16(pretrained=True, **args)\n",
    "        self.model.eval().to(device)\n",
    "\n",
    "        self.layers = listify(layers)\n",
    "        self.weights = listify(weights)\n",
    "\n",
    "        if loss == \"mse\":\n",
    "            self.criterion = nn.MSELoss()\n",
    "        elif loss == \"mae\":\n",
    "            self.criterion = nn.L1Loss()\n",
    "        else:\n",
    "            raise KeyError\n",
    "\n",
    "    def forward(self, input, style):\n",
    "        \"\"\"\n",
    "        Measure distance between feature representations of input and content images\n",
    "        \"\"\"\n",
    "        input_features = self.get_features(input)\n",
    "        style_features = self.get_features(style)\n",
    "        # print(style_features[0].size(), len(style_features))\n",
    "\n",
    "        input_gram = [self.gram_matrix(x) for x in input_features]\n",
    "        style_gram = [self.gram_matrix(x) for x in style_features]\n",
    "\n",
    "        loss = [\n",
    "            self.criterion(torch.stack(i_g), torch.stack(s_g)) for i_g, s_g in zip(input_gram, style_gram)\n",
    "        ]\n",
    "        return torch.mean(torch.tensor(loss))\n",
    "\n",
    "    def get_features(self, x):\n",
    "        \"\"\"\n",
    "        Extract feature maps from the intermediate layers.\n",
    "        \"\"\"\n",
    "        if self.layers is None:\n",
    "            self.layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "        features = []\n",
    "        for name, module in self.model.features._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layers:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "    def gram_matrix(self, input):\n",
    "        \"\"\"\n",
    "        Compute Gram matrix for each image in batch\n",
    "        input: Tensor of shape BxCxHxW\n",
    "            B: batch size\n",
    "            C: channels size\n",
    "            H&W: spatial size\n",
    "        \"\"\"\n",
    "\n",
    "        B, C, H, W = input.size()\n",
    "        gram = []\n",
    "        for i in range(B):\n",
    "            x = input[i].view(C, H * W)\n",
    "            gram.append(torch.mm(x, x.t()))\n",
    "        return gram"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
