{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T15:40:37.231851Z",
     "start_time": "2020-07-01T15:40:37.111752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Development.ipynb  Tests.ipynb\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:45:36.207667Z",
     "start_time": "2020-07-06T07:45:34.066510Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random \n",
    "import functools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as albu_pt\n",
    "import piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:45:37.738377Z",
     "start_time": "2020-07-06T07:45:37.325888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 06 2020 \n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.0\n",
      "torch 1.6.0.dev20200610\n",
      "albumentations 0.4.5\n",
      "photosynthesis_metrics not installed\n",
      "\n",
      "compiler   : GCC 8.4.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-108-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "# Fix to be able to import python modules inside a notebook\n",
    "os.chdir('..')\n",
    "\n",
    "# Useful extensions\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,torch,albumentations,photosynthesis_metrics\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Nice plot formating\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T07:59:17.044998Z",
     "start_time": "2020-07-06T07:59:17.038890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([4, 4, 2])\n",
      "torch.Size([4, 4]) torch.Size([4, 4])\n",
      "tensor([[ 0.1275,  0.1148,  0.4154,  0.1148],\n",
      "        [ 0.5056,  0.0122, -0.0510, -0.1064],\n",
      "        [-0.2891, -0.1570,  0.0370, -0.1570],\n",
      "        [ 0.5056, -0.1064, -0.0510,  0.0122]]) \n",
      " tensor([[ 0.3083,  0.0786,  0.3339,  0.0786],\n",
      "        [ 0.4727, -0.1318, -0.0180,  0.0376],\n",
      "        [-0.2615, -0.1208, -0.0899, -0.1208],\n",
      "        [ 0.4727,  0.0376, -0.0180, -0.1318]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "# y = torch.rfft(x, 2, onesided=False)\n",
    "y = torch.stack([x, torch.zeros_like(x)], dim=-1)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "ifft = torch.ifft(y, 2)[..., 0]\n",
    "irfft = torch.irfft(y, 2, onesided=False)\n",
    "\n",
    "print(ifft.shape, irfft.shape)\n",
    "print(ifft, \"\\n\", irfft)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FSIM and FSIMc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:38:51.646494Z",
     "start_time": "2020-07-01T22:38:51.606182Z"
    },
    "code_folding": [
     8,
     107,
     200
    ]
   },
   "outputs": [],
   "source": [
    "r\"\"\"Implemetation of Feature Similarity Index Measure\n",
    "Code is based on MATLAB version for computations in pixel domain\n",
    "https://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/Files/FeatureSIM.m\n",
    "References:\n",
    "    https://www4.comp.polyu.edu.hk/~cslzhang/IQA/TIP_IQA_FSIM.pdf\n",
    "\"\"\"\n",
    "import math\n",
    "import functools\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "from piq.utils import _adjust_dimensions, _validate_input\n",
    "from piq.functional import ifftshift, get_meshgrid, similarity_map, gradient_map, scharr_filter\n",
    "\n",
    "\n",
    "def fsim(x: torch.Tensor, y: torch.Tensor, reduction: str = 'mean',\n",
    "         data_range: Union[int, float] = 1.0, chromatic: bool = True,\n",
    "         scales: int = 4, orientations: int = 4, min_length: int = 6,\n",
    "         mult: int = 2, sigma_f: float = 0.55, delta_theta: float = 1.2,\n",
    "         k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Feature Similarity Index Measure for a batch of images.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        x: Batch of predicted images with shape (batch_size x channels x H x W)\n",
    "        y: Batch of target images with shape  (batch_size x channels x H x W)\n",
    "        data_range: Value range of input images (usually 1.0 or 255). Default: 1.0\n",
    "        chromatic: Flag to compute FSIMc, which also takes into account chromatic components\n",
    "        scales: Number of wavelets used for computation of phase congruensy maps\n",
    "        orientations: Number of filter orientations used for computation of phase congruensy maps\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian describing the log Gabor filter's\n",
    "            transfer function in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations and the standard deviation\n",
    "            of the angular Gaussian function used to construct filters in the frequency plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean at which we set the noise\n",
    "            threshold  point, below which phase congruency values get penalized.\n",
    "        \n",
    "    Returns:\n",
    "        FSIM: Index of similarity betwen two images. Usually in [0, 1] interval.\n",
    "            Can be bigger than 1 for predicted images with higher contrast than original one.\n",
    "    Note:\n",
    "        This implementation is based on original authors MATLAB code.\n",
    "        https://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/FSIM.htm\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    _validate_input(input_tensors=(x, y), allow_5d=False)\n",
    "    x, y = _adjust_dimensions(input_tensors=(x, y))\n",
    "    \n",
    "    # Scale to [0., 1.] range\n",
    "    x = x / float(data_range)\n",
    "    y = y / float(data_range)\n",
    "    \n",
    "    # Rescale to [0, 255] range, because all constant are calculated for this factor\n",
    "    x = x * 255\n",
    "    y = y * 255\n",
    "    \n",
    "    # Apply average pooling\n",
    "    kernel_size = max(1, round(min(x.shape[-2:]) / 256))\n",
    "    x = torch.nn.functional.avg_pool2d(x, kernel_size, stride=2)\n",
    "    y = torch.nn.functional.avg_pool2d(y, kernel_size, stride=2)\n",
    "        \n",
    "    num_channels = x.size(1)\n",
    "\n",
    "    # Convert RGB to YIQ color space https://en.wikipedia.org/wiki/YIQ\n",
    "    if num_channels == 3:\n",
    "        yiq_weights = torch.tensor([\n",
    "            [0.299, 0.587, 0.114],\n",
    "            [0.5959, -0.2746, -0.3213],\n",
    "            [0.2115, -0.5227, 0.3112]]).t().to(x)\n",
    "        x = torch.matmul(x.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        y = torch.matmul(y.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_lum = x[:, : 1, :, :]\n",
    "        y_lum = y[:, : 1, :, :]\n",
    "        \n",
    "        x_i = x[:, 1, :, :]\n",
    "        y_i = y[:, 1, :, :]\n",
    "        x_q = x[:, 2, :, :]\n",
    "        y_q = y[:, 2, :, :]\n",
    "\n",
    "    else:\n",
    "        x_lum = x\n",
    "        y_lum = y\n",
    "\n",
    "    # Compute phase congruency maps\n",
    "    pc_x = _phase_congruency(\n",
    "        x_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    pc_y = _phase_congruency(\n",
    "        y_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    \n",
    "    # Gradient maps\n",
    "    kernels = torch.stack([scharr_filter(), scharr_filter().transpose(-1, -2)])\n",
    "    grad_map_x = gradient_map(x_lum, kernels)\n",
    "    grad_map_y = gradient_map(y_lum, kernels)\n",
    "    \n",
    "    # Constants from paper\n",
    "    T1, T2, T3, T4, lmbda = 0.85, 160, 200, 200, 0.03\n",
    "    \n",
    "    # Compute FSIM\n",
    "    PC = similarity_map(pc_x, pc_y, T1)\n",
    "    GM = similarity_map(grad_map_x, grad_map_y, T2)\n",
    "    pc_max = torch.where(pc_x > pc_y, pc_x, pc_y)\n",
    "    score = GM * PC * pc_max\n",
    "    \n",
    "    if chromatic:\n",
    "        S_I = similarity_map(x_i, y_i, T3)\n",
    "        S_Q = similarity_map(x_q, y_q, T4)\n",
    "        score = score * torch.abs(S_I * S_Q) ** lmbda\n",
    "        # Complex gradients will work in PyTorch 1.6.0\n",
    "        # score = score * torch.real((S_I * S_Q).to(torch.complex64) ** lmbda)\n",
    "\n",
    "    result = score.sum(dim=[1, 2, 3]) / pc_max.sum(dim=[1, 2, 3])\n",
    "    \n",
    "    if reduction == 'none':\n",
    "        return result\n",
    "\n",
    "    return {'mean': result.mean,\n",
    "            'sum': result.sum\n",
    "            }[reduction](dim=0)\n",
    "\n",
    "\n",
    "def _construct_filters(x: torch.Tensor, scales: int = 4, orientations: int = 4,\n",
    "                       min_length: int = 6, mult: int = 2, sigma_f: float = 0.55,\n",
    "                       delta_theta: float = 1.2, k: float = 2.0):\n",
    "    \"\"\"Creates stack of filters used for computation of phase congruensy maps\n",
    "    \n",
    "    Args:\n",
    "        x: Tensor with shape Bx1xHxW\n",
    "        scales: Number of wavelets\n",
    "        orientations: Number of filter orientations\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian\n",
    "            describing the log Gabor filter's transfer function\n",
    "            in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations\n",
    "            and the standard deviation of the angular Gaussian function\n",
    "            used to construct filters in the freq. plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean\n",
    "            at which we set the noise threshold point, below which phase\n",
    "            congruency values get penalized.\n",
    "        \"\"\"\n",
    "    B, _, H, W = x.shape\n",
    "\n",
    "    # Calculate the standard deviation of the angular Gaussian function\n",
    "    # used to construct filters in the freq. plane.\n",
    "    theta_sigma = math.pi / (orientations * delta_theta)\n",
    "\n",
    "    # Pre-compute some stuff to speed up filter construction\n",
    "    grid_x, grid_y = get_meshgrid((H, W))\n",
    "    radius = torch.sqrt(grid_x ** 2 + grid_y ** 2)\n",
    "    theta = torch.atan2(-grid_y, grid_x)\n",
    "\n",
    "    # Quadrant shift radius and theta so that filters are constructed with 0 frequency at the corners.\n",
    "    # Get rid of the 0 radius value at the 0 frequency point (now at top-left corner)\n",
    "    # so that taking the log of the radius will not cause trouble.\n",
    "    radius = ifftshift(radius)\n",
    "    theta = ifftshift(theta)\n",
    "    radius[0, 0] = 1\n",
    "\n",
    "    sintheta = torch.sin(theta)\n",
    "    costheta = torch.cos(theta)\n",
    "\n",
    "    # Filters are constructed in terms of two components.\n",
    "    # 1) The radial component, which controls the frequency band that the filter responds to\n",
    "    # 2) The angular component, which controls the orientation that the filter responds to.\n",
    "    # The two components are multiplied together to construct the overall filter.\n",
    "\n",
    "    # First construct a low-pass filter that is as large as possible, yet falls\n",
    "    # away to zero at the boundaries.  All log Gabor filters are multiplied by\n",
    "    # this to ensure no extra frequencies at the 'corners' of the FFT are\n",
    "    # incorporated as this seems to upset the normalisation process when\n",
    "    lp = _lowpassfilter(size=(H, W), cutoff=.45, n=15)\n",
    "\n",
    "    # Construct the radial filter components...\n",
    "    log_gabor = []\n",
    "    for s in range(scales):\n",
    "        wavelength = min_length * mult ** s\n",
    "        fo = 1.0 / wavelength  # Centre frequency of filter.\n",
    "        gabor_filter = torch.exp((- torch.log(radius / fo) ** 2) / (2 * math.log(sigma_f) ** 2))\n",
    "        gabor_filter = gabor_filter * lp\n",
    "        gabor_filter[0, 0] = 0\n",
    "        log_gabor.append(gabor_filter)\n",
    "\n",
    "    # Then construct the angular filter components...\n",
    "    spread = []\n",
    "    for o in range(orientations):\n",
    "        angl = o * math.pi / orientations\n",
    "\n",
    "        # For each point in the filter matrix calculate the angular distance from\n",
    "        # the specified filter orientation.  To overcome the angular wrap-around\n",
    "        # problem sine difference and cosine difference values are first computed\n",
    "        # and then the atan2 function is used to determine angular distance.\n",
    "        ds = sintheta * math.cos(angl) - costheta * math.sin(angl)  # Difference in sine.\n",
    "        dc = costheta * math.cos(angl) + sintheta * math.sin(angl)  # Difference in cosine.\n",
    "        dtheta = torch.abs(torch.atan2(ds, dc))\n",
    "        spread.append(torch.exp((- dtheta ** 2) / (2 * theta_sigma ** 2)))\n",
    "\n",
    "    spread = torch.stack(spread)\n",
    "    log_gabor = torch.stack(log_gabor)\n",
    "    \n",
    "    # Multiply, add batch dimension and transfer to correct device.\n",
    "    filters = (spread.repeat_interleave(scales, dim=0) * log_gabor.repeat(orientations, 1, 1)).unsqueeze(0).to(x)\n",
    "    return filters\n",
    "\n",
    "\n",
    "def _phase_congruency(x: torch.Tensor, scales: int = 4, orientations: int = 4,\n",
    "                      min_length: int = 6, mult: int = 2, sigma_f: float = 0.55,\n",
    "                      delta_theta: float = 1.2, k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Phase Congruence for a batch of greyscale images\n",
    "\n",
    "    Args:\n",
    "        x: Tensor with shape Bx1xHxW\n",
    "        levels: Number of wavelet scales\n",
    "        orientations: Number of filter orientations\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian\n",
    "            describing the log Gabor filter's transfer function\n",
    "            in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations\n",
    "            and the standard deviation of the angular Gaussian function\n",
    "            used to construct filters in the freq. plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean\n",
    "            at which we set the noise threshold point, below which phase\n",
    "            congruency values get penalized.\n",
    "    Returns:\n",
    "        PCmap: Tensor with shape BxHxW\n",
    "\n",
    "    \"\"\"\n",
    "    EPS = 1e-4\n",
    "\n",
    "    B, _, H, W = x.shape\n",
    "\n",
    "    # Fourier transform\n",
    "    imagefft = torch.rfft(x, 2, onesided=False)\n",
    "\n",
    "    filters = _construct_filters(x, scales, orientations, min_length, mult, sigma_f, delta_theta, k)\n",
    "\n",
    "    # Note rescaling to match power record ifft2 of filter\n",
    "    filters_ifft = torch.ifft(torch.stack([filters, torch.zeros_like(filters)], dim=-1), 2)[..., 0] * math.sqrt(H * W)\n",
    "    \n",
    "    # Convolve image with even and odd filters\n",
    "    E0 = torch.ifft(imagefft * filters.unsqueeze(-1), 2).view(B, orientations, scales, H, W, 2)\n",
    "\n",
    "    # Amplitude of even & odd filter response. An = sqrt(real^2 + imag^2)\n",
    "    an = torch.sqrt(torch.sum(E0 ** 2, dim=-1))\n",
    "\n",
    "    # Take filter at scale 0 and sum spatially\n",
    "    # Record mean squared filter value at smallest scale.\n",
    "    # This is used for noise estimation.\n",
    "    em_n = (filters.view(1, orientations, scales, H, W)[:, :, :1, ...] ** 2).sum(dim=[-2, -1], keepdims=True)\n",
    "\n",
    "    # Sum of even filter convolution results.\n",
    "    sum_e = E0[..., 0].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Sum of odd filter convolution results.\n",
    "    sum_o = E0[..., 1].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Get weighted mean filter response vector, this gives the weighted mean phase angle.\n",
    "    x_energy = torch.sqrt(sum_e ** 2 + sum_o ** 2) + EPS\n",
    "\n",
    "    mean_e = sum_e / x_energy\n",
    "    mean_o = sum_o / x_energy\n",
    "\n",
    "    # Now calculate An(cos(phase_deviation) - | sin(phase_deviation)) | by\n",
    "    # using dot and cross products between the weighted mean filter response\n",
    "    # vector and the individual filter response vectors at each scale.\n",
    "    # This quantity is phase congruency multiplied by An, which we call energy.\n",
    "\n",
    "    # Extract even and odd convolution results.\n",
    "    E = E0[..., 0]\n",
    "    O = E0[..., 1]\n",
    "\n",
    "    energy = (E * mean_e + O * mean_o - torch.abs(E * mean_o - O * mean_e)).sum(dim=2, keepdim=True)\n",
    "    \n",
    "    # Compensate for noise\n",
    "    # We estimate the noise power from the energy squared response at the\n",
    "    # smallest scale.  If the noise is Gaussian the energy squared will have a\n",
    "    # Chi-squared 2DOF pdf.  We calculate the median energy squared response\n",
    "    # as this is a robust statistic.  From this we estimate the mean.\n",
    "    # The estimate of noise power is obtained by dividing the mean squared\n",
    "    # energy value by the mean squared filter value\n",
    "    \n",
    "    abs_e0 = torch.sqrt(torch.sum(E0[:, :, :1, ...] ** 2, dim=-1)).reshape(B, orientations, 1, 1, H * W)\n",
    "    median_e2n = torch.median(abs_e0 ** 2, dim=-1, keepdims=True).values\n",
    "\n",
    "    mean_e2n = - median_e2n / math.log(0.5)\n",
    "\n",
    "    # Estimate of noise power.\n",
    "    noisePower = mean_e2n / em_n\n",
    "    \n",
    "    # Now estimate the total energy^2 due to noise\n",
    "    # Estimate for sum(An^2) + sum(Ai.*Aj.*(cphi.*cphj + sphi.*sphj))\n",
    "    filters_ifft = filters_ifft.view(1, orientations, scales, H, W)\n",
    "    \n",
    "    sum_an2 = torch.sum(filters_ifft ** 2, dim=-3, keepdim=True)\n",
    "    \n",
    "    sum_ai_aj = torch.zeros(B, orientations, 1, H, W).to(x)\n",
    "    for s in range(scales - 1):\n",
    "        sum_ai_aj = sum_ai_aj + (filters_ifft[:, :, s: s + 1] * filters_ifft[:, :, s + 1:]).sum(dim=-3, keepdim=True)\n",
    "            \n",
    "    sum_an2 = torch.sum(sum_an2, dim=[-1, -2], keepdim=True)\n",
    "    sum_ai_aj = torch.sum(sum_ai_aj, dim=[-1, -2], keepdim=True)\n",
    "\n",
    "    noise_energy2 = 2 * noisePower * sum_an2 + 4 * noisePower * sum_ai_aj\n",
    "\n",
    "    # Rayleigh parameter\n",
    "    tau = torch.sqrt(noise_energy2 / 2)\n",
    "\n",
    "    # Expected value of noise energy\n",
    "    noise_energy = tau * math.sqrt(math.pi / 2)\n",
    "    moise_energy_sigma = torch.sqrt((2 - math.pi / 2) * tau ** 2)\n",
    "\n",
    "    # Noise threshold\n",
    "    T = noise_energy + k * moise_energy_sigma\n",
    "\n",
    "    # The estimated noise effect calculated above is only valid for the PC_1 measure.\n",
    "    # The PC_2 measure does not lend itself readily to the same analysis.  However\n",
    "    # empirically it seems that the noise effect is overestimated roughly by a factor\n",
    "    # of 1.7 for the filter parameters used here.\n",
    "\n",
    "    # Empirical rescaling of the estimated noise effect to suit the PC_2 phase congruency measure\n",
    "    T = T / 1.7\n",
    "\n",
    "    # Apply noise threshold\n",
    "    energy = torch.max(energy - T, torch.zeros_like(T))\n",
    "\n",
    "    eps = torch.finfo(energy.dtype).eps\n",
    "    energy_all = energy.sum(dim=[1, 2]) + eps\n",
    "    an_all = an.sum(dim=[1, 2]) + eps\n",
    "    result_pc = energy_all / an_all\n",
    "    return result_pc.unsqueeze(1)\n",
    "\n",
    "\n",
    "def _lowpassfilter(size: Union[int, Tuple[int, int]], cutoff: float, n: int) -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Constructs a low-pass Butterworth filter.\n",
    "    Args:\n",
    "        size: Tuple with heigth and width of filter to construct\n",
    "        cutoff: Cutoff frequency of the filter in (0, 0.5()\n",
    "        n: Filter order. Higher `n` means sharper transition.\n",
    "            Note that `n` is doubled so that it is always an even integer.\n",
    "        \n",
    "    Returns:\n",
    "        f = 1 / (1 + w/cutoff) ^ 2n\n",
    "    \n",
    "    Note:\n",
    "        The frequency origin of the returned filter is at the corners.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    assert (cutoff >= 0) and (cutoff <= 0.5), \"Cutoff frequency must be between 0 and 0.5\"\n",
    "    assert n > 1, \"n must be an integer >= 1\"\n",
    "\n",
    "    grid_x, grid_y = get_meshgrid(size)\n",
    "    \n",
    "    # A matrix with every pixel = radius relative to centre.\n",
    "    radius = torch.sqrt(grid_x ** 2 + grid_y ** 2)\n",
    "\n",
    "    return ifftshift(1. / (1.0 + (radius / cutoff) ** (2 * n)))\n",
    "\n",
    "\n",
    "class FSIMLoss(_Loss):\n",
    "    r\"\"\"Creates a criterion that measures the FSIM or FSIMc for input :math:`x` and target :math:`y`.\n",
    "\n",
    "    In order to be considered as a loss, value `1 - clip(FSIM, min=0, max=1)` is returned. If you need FSIM value,\n",
    "    use function `fsim` instead.\n",
    "\n",
    "    Args:\n",
    "        chromatic: Flag to compute FSIMc, which also takes into account chromatic components\n",
    "        reduction: Specifies the reduction to apply to the output:\n",
    "            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
    "            ``'mean'``: the sum of the output will be divided by the number of\n",
    "            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
    "        data_range: The difference between the maximum and minimum of the pixel value,\n",
    "            i.e., if for image x it holds min(x) = 0 and max(x) = 1, then data_range = 1.\n",
    "            The pixel value interval of both input and output should remain the same.\n",
    "        scales: Number of wavelets used for computation of phase congruensy maps\n",
    "        orientations: Number of filter orientations used for computation of phase congruensy maps\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian describing the log Gabor filter's\n",
    "            transfer function in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations and the standard deviation\n",
    "            of the angular Gaussian function used to construct filters in the frequency plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean at which we set the noise\n",
    "            threshold  point, below which phase congruency values get penalized.\n",
    "\n",
    "    Shape:\n",
    "        - Input: Required to be 2D (H, W), 3D (C,H,W), 4D (N,C,H,W) or 5D (N,C,H,W,2), channels first.\n",
    "        - Target: Required to be 2D (H, W), 3D (C,H,W), 4D (N,C,H,W) or 5D (N,C,H,W,2), channels first.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> loss = FSIMLoss()\n",
    "        >>> prediction = torch.rand(3, 3, 256, 256, requires_grad=True)\n",
    "        >>> target = torch.rand(3, 3, 256, 256)\n",
    "        >>> output = loss(prediction, target)\n",
    "        >>> output.backward()\n",
    "\n",
    "    References:\n",
    "        .. [1] Anish Mittal et al. \"No-Reference Image Quality Assessment in the Spatial Domain\",\n",
    "        https://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf\n",
    "        \"\"\"\n",
    "    def __init__(self, data_range: Union[int, float] = 1., reduction: str = 'mean', scales: int = 4,\n",
    "                 orientations: int = 4, min_length: int = 6, mult: int = 2, sigma_f: float = 0.55,\n",
    "                 delta_theta: float = 1.2, k: float = 2.0) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "        self.data_range = data_range\n",
    "        self.reduction = reduction\n",
    "\n",
    "        # Save function with predefined parameters, rather than parameters themself\n",
    "        self.fsim = functools.partial(\n",
    "            fsim,\n",
    "            data_range=data_range,\n",
    "            reduction=reduction,\n",
    "            scales=scales,\n",
    "            orientations=orientations,\n",
    "            min_length=min_length,\n",
    "            mult=mult,\n",
    "            sigma_f=sigma_f,\n",
    "            delta_theta=delta_theta,\n",
    "            k=k,\n",
    "        )\n",
    "\n",
    "    def forward(self, prediction: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computation of FSIM as a loss function.\n",
    "        Args:\n",
    "            prediction: Tensor of prediction of the network.\n",
    "            target: Reference tensor.\n",
    "        Returns:\n",
    "            Value of FSIM loss to be minimized. 0 <= FSIM <= 1.\n",
    "        \"\"\"\n",
    "        # All checks are done inside fsim function\n",
    "        score = self.fsim(prediction, target)\n",
    "\n",
    "        # Make sure value to be in [0, 1] range and convert to loss\n",
    "        return 1 - torch.clamp(score, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:38:53.055455Z",
     "start_time": "2020-07-01T22:38:52.493005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7759, 0.7812, 0.7768, 0.7776])\n"
     ]
    }
   ],
   "source": [
    "# x = torch.zeros(4, 3, 128, 128)\n",
    "# y = torch.zeros(4, 3, 128, 128)\n",
    "x = torch.rand(4, 3, 128, 128)\n",
    "y = torch.rand(4, 3, 128, 128)\n",
    "\n",
    "# x = torch.ones(4, 3, 128, 128)\n",
    "# y = torch.ones(4, 3, 128, 128)\n",
    "print(fsim(x, y, reduction='none', chromatic=False))\n",
    "\n",
    "# print(fsim_bad(x, y, reduction='none', chromatic=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:49:11.950202Z",
     "start_time": "2020-07-01T16:49:11.935073Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VSI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b25b29343aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mFSIM\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.89691\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFSIMc\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0.89691\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mVSI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# goldhill <-> goldhill_blur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VSI' is not defined"
     ]
    }
   ],
   "source": [
    "# goldhill <-> goldhill_jpeg\n",
    "FSIM =  0.89691\n",
    "FSIMc =  0.89691\n",
    "VSI\n",
    "\n",
    "# goldhill <-> goldhill_blur\n",
    "FSIM =  0.91063\n",
    "FSIMc =  0.91063\n",
    "\n",
    "\n",
    "# I01 <-> i01_01_5\n",
    "FSIM =  0.93674\n",
    "FSIMc =  0.92587\n",
    "VSI =  0.96405\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:47:10.228131Z",
     "start_time": "2020-07-01T16:47:05.921703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1_01_5 FSIM tensor(0.9367)\n",
      "i1_01_5 FSIMc tensor(0.9259)\n",
      "goldhill_jpeg FSIM tensor(0.8969)\n"
     ]
    }
   ],
   "source": [
    "# Read images\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "\n",
    "I01 = torch.tensor(imread('data/external/I01.BMP')).permute(2, 0, 1)\n",
    "i1_01_5 = torch.tensor(imread('data/external/i01_01_5.bmp')).permute(2, 0, 1)\n",
    "\n",
    "goldhill = torch.tensor(imread('data/external/goldhill.gif'))\n",
    "goldhill_jpeg = torch.tensor(imread('data/external/goldhill_jpeg.gif'))\n",
    "\n",
    "\n",
    "\n",
    "print(\"i1_01_5 FSIM\", fsim(i1_01_5, I01, data_range=255, chromatic=False))\n",
    "print(\"i1_01_5 FSIMc\", fsim(i1_01_5, I01, data_range=255, chromatic=True))\n",
    "\n",
    "print(\"goldhill_jpeg FSIM\", fsim(goldhill_jpeg, goldhill, data_range=255, chromatic=False))\n",
    "\n",
    "# fsim = \n",
    "# ------------------\n",
    "# image1 = mpimg.imread('data/external/I01.BMP')\n",
    "# image2 = mpimg.imread('data/external/i01_01_5.bmp')\n",
    "\n",
    "# Convert to tensor and create fake batch\n",
    "# image1_t_cuda = torch.tensor(image1).permute(2, 0, 1).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).permute(2, 0, 1).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image3).permute(2, 0, 1).unsqueeze(0).cuda()\n",
    "# print(\"Input image shape:\", image1_t_cuda.shape)\n",
    "\n",
    "# image1_t_cuda.requires_grad_()\n",
    "# loss = fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1, chromatic=True)\n",
    "# loss.backward()\n",
    "\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1).cuda() / 255.\n",
    "\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "\n",
    "# fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1.0, chromatic=False)\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "\n",
    "# image1_t_cuda = torch.rand(3, 3, 8, 8)\n",
    "# image2_t_cuda = torch.rand(3, 3, 8, 8)\n",
    "# fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1.0, chromatic=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:49:51.980128Z",
     "start_time": "2020-07-01T16:49:51.946061Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T21:56:39.910219Z",
     "start_time": "2020-07-01T21:56:39.907556Z"
    }
   },
   "outputs": [],
   "source": [
    "from piq import fsim, psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:21:27.833929Z",
     "start_time": "2020-07-01T22:21:27.798726Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:21:29.325862Z",
     "start_time": "2020-07-01T22:21:28.853518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1123, 2.9341, 2.9150, 3.0244])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.zeros(4, 3, 128, 128)\n",
    "# y = torch.zeros(4, 3, 128, 128)\n",
    "x = torch.rand(4, 3, 128, 128)\n",
    "y = torch.rand(4, 3, 128, 128)\n",
    "\n",
    "# x = torch.ones(4, 3, 128, 128)\n",
    "# y = torch.ones(4, 3, 128, 128)\n",
    "fsim(x, y, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T19:35:28.337801Z",
     "start_time": "2020-07-01T19:35:28.314338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test different Log Gabor filters\n",
    "\n",
    "size = (1024, 512)\n",
    "def get_meshgrid(size: Tuple[int, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        size: Shape of meshgrid to create\n",
    "    \"\"\"\n",
    "    if size[0] % 2:\n",
    "        # Odd\n",
    "        x = torch.range(-(size[0] - 1) / 2, (size[0] - 1) / 2) / (size[0] - 1)\n",
    "    else:\n",
    "        # Even\n",
    "        x = torch.range(- size[0] / 2, size[0] / 2 - 1) / size[0]\n",
    "    \n",
    "    if size[1] % 2:\n",
    "        # Odd\n",
    "        y = torch.range(-(size[1] - 1) / 2, (size[1] - 1) / 2) / (size[1] - 1)\n",
    "    else:\n",
    "        # Even\n",
    "        y = torch.range(- size[1] / 2, size[1] / 2 - 1) / size[1]\n",
    "    return torch.meshgrid(x, y)\n",
    "\n",
    "# # lp = _lowpassfilter(size=(H, W), cutoff=.45, n=15)\n",
    "# sigma_y = sigma/gamma\n",
    "# g0 = torch.exp(-0.5 * ((x1**2 / sigma**2) + (y1**2 / sigma_y**2)))\n",
    "        \n",
    "        \n",
    "def log_gabor_filter(size: Tuple[int, int], omega_0: float, sigma_f: float) -> torch.Tensor:\n",
    "    \"\"\"Constructs log Gabor filter of given shape. \n",
    "    \"\"\"\n",
    "    grid_x, grid_y = get_meshgrid(size)\n",
    "    \n",
    "    radius = torch.sqrt(grid_x ** 2 + grid_y ** 2)\n",
    "    radius = ifftshift(radius)\n",
    "    radius[0, 0] = 1\n",
    "    \n",
    "    gabor_filter = torch.exp((- torch.log(radius / omega_0) ** 2) / (2 * math.log(sigma_f) ** 2))\n",
    "    return gabor_filter\n",
    "\n",
    "\n",
    "def log_gabor_filter2(size: Tuple[int, int], omega_0: float = 0.021, sigma_f: float = 1.34) -> torch.Tensor:\n",
    "    \n",
    "    xx, yy = get_meshgrid(size)\n",
    "\n",
    "    mask = xx.pow(2) + yy.pow(2) <= 0.25\n",
    "    xx = xx * mask\n",
    "    yy = yy * mask\n",
    "\n",
    "    xx = ifftshift(xx)\n",
    "    yy = ifftshift(yy)\n",
    "\n",
    "    r = (xx.pow(2) + yy.pow(2)).sqrt()\n",
    "    r[0, 0] = 1\n",
    "    \n",
    "    lg = torch.exp((- torch.log(r / omega_0) ** 2) / (2 * math.log(sigma_f) ** 2))\n",
    "\n",
    "    lg[0, 0] = 0\n",
    "    return lg\n",
    "\n",
    "# lg = log_gabor_filter(size, omega_0=0.021, sigma_f=1.34)\n",
    "\n",
    "xx, yy = get_meshgrid(size)\n",
    "mask = xx.pow(2) + yy.pow(2) <= 0.25\n",
    "\n",
    "# lg = lg # * mask\n",
    "\n",
    "# lg2 = log_gabor_filter2(size, omega_0=0.021, sigma_f=1.34)\n",
    "\n",
    "# torch.sum(lg - lg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T19:35:29.521956Z",
     "start_time": "2020-07-01T19:35:29.410281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec9cb38e80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:54:38.216609Z",
     "start_time": "2020-07-01T16:54:37.463371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1_01_5 VSI tensor(0.9521)\n"
     ]
    }
   ],
   "source": [
    "print(\"i1_01_5 VSI\", vsi(I01, i1_01_5, data_range=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T12:24:03.103357Z",
     "start_time": "2020-04-26T12:24:03.099630Z"
    }
   },
   "outputs": [],
   "source": [
    "# mnist = torchvision.datasets.MNIST(\"../datasets\", train=True, download=True)\n",
    "# cifar10 = torchvision.datasets.CIFAR10(\"../datasets\", download=True)\n",
    "# cifar100 = torchvision.datasets.CIFAR100(\"../datasets\", download=True)\n",
    "# fashin_mnist = torchvision.datasets.FashionMNIST(\"datasets\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T17:41:48.647750Z",
     "start_time": "2020-06-03T17:41:24.437192Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -P data/raw http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !wget -P data/raw http://vllab.ucmerced.edu/wlai24/LapSRN/results/SR_testing_datasets.zip\n",
    "# !wget -P data/raw http://www.cs.columbia.edu/CAVE/databases/SLAM_coil-20_coil-100/coil-100/coil-100.zip\n",
    "# !wget -P data/raw http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip\n",
    "# !wget -P data/raw http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip\n",
    "# !wget -P data/raw http://www.ponomarenko.info/tid2013/tid2013.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T11:49:48.118824Z",
     "start_time": "2020-06-25T11:49:33.826033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-25 14:49:33--  http://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip\n",
      "Resolving datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)... 134.34.224.175\n",
      "Connecting to datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)|134.34.224.175|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip [following]\n",
      "--2020-06-25 14:49:33--  https://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip\n",
      "Connecting to datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)|134.34.224.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 766965996 (731M) [application/zip]\n",
      "Saving to: ‘datasets/koniq10k_512x384.zip’\n",
      "\n",
      "koniq10k_512x384.zi 100%[===================>] 731.44M  54.4MB/s    in 14s     \n",
      "\n",
      "2020-06-25 14:49:48 (53.3 MB/s) - ‘datasets/koniq10k_512x384.zip’ saved [766965996/766965996]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget -P data/raw http://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T11:50:24.625456Z",
     "start_time": "2020-06-25T11:50:24.503496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdatasets\u001b[0m/  Development.ipynb  Tests.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TID2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:07:18.946020Z",
     "start_time": "2020-06-11T12:07:18.929999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'albu_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ad52812453c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malbu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0malbu_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'albu_pt' is not defined"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import TID2013\n",
    "\n",
    "transform = albu.Compose([\n",
    "#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\n",
    "    albu.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.]), # to [0, 1]\n",
    "    albu_pt.ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = TID2013(transform=transform)\n",
    "distorted_images, reference_images, scores = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:50:26.291463Z",
     "start_time": "2020-06-08T08:50:26.122445Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.augmentations import get_aug\n",
    "from src.utils import walk_files\n",
    "from src.datasets import *\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:53:13.529231Z",
     "start_time": "2020-06-08T08:50:31.586592Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c99e75bb6d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repoz/metrics-comparison/src/augmentations.py\u001b[0m in \u001b[0;36mget_aug\u001b[0;34m(aug_type, task, dataset, size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Add the same noise for all channels for single-channel images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMEAN_STD_BY_NAME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"medicaldecathlon\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msinglechannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "TASK = \"denoise\"\n",
    "SIZE = 256\n",
    "\n",
    "dataset_class = {\n",
    "    \"mnist\": MNIST,\n",
    "    \"fashion_mnist\": FashionMNIST, \n",
    "    \"cifar10\": CIFAR10,\n",
    "    \"cifar100\": CIFAR100,\n",
    "    \"tinyimagenet\": TinyImageNet,\n",
    "    \"div2k\": DIV2K,\n",
    "    \"set5\": Set5,\n",
    "    \"set14\": Set14,\n",
    "    \"urba100\": Urban100,\n",
    "    \"manga109\": Manga109,\n",
    "    \"coil100\": COIL100,\n",
    "    \"bsds100\": BSDS100,\n",
    "    \"medicaldecathlon\": MedicalDecathlon\n",
    "}\n",
    "\n",
    "\n",
    "dataset_names = [\n",
    "    \"div2k\",\n",
    "    \"bsds100\",\n",
    "#     \"set5\"\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for dataset_name in dataset_names:\n",
    "    transform = get_aug(aug_type=\"medium\", task=TASK, dataset=datasets, size=SIZE)\n",
    "    datasets.append(dataset_class[dataset_name](train=True, transform=transform))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    input, target = dataset[1]\n",
    "    print(\"Input\", input.shape, input.min(), input.max(), input.mean())\n",
    "    print(\"Target\", target.shape, target.min(), target.max(), target.mean())\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0) * 0.5 + 0.5, )\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:03:51.120720Z",
     "start_time": "2020-05-29T13:03:51.118706Z"
    }
   },
   "outputs": [],
   "source": [
    "# total_mean, total_var = [], []\n",
    "# for i in range(len(div2k)):\n",
    "#     image = div2k[i][1]\n",
    "#     total_mean.append(image.mean(dim=[1,2]))\n",
    "#     total_var.append(image.var(dim=[1,2]))\n",
    "# print(torch.stack(total_mean).mean(dim=0))\n",
    "# print(torch.stack(total_var).mean(dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T15:59:56.557719Z",
     "start_time": "2020-05-29T15:59:55.444859Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:28:43.539258Z",
     "start_time": "2020-05-29T12:28:28.385984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"denoise\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:59:14.735977Z",
     "start_time": "2020-04-26T14:58:51.307570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"deblur\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:00:22.313189Z",
     "start_time": "2020-04-26T15:00:22.300261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(image.min(), image.max(), image.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:02.588910Z",
     "start_time": "2020-04-26T10:46:02.235819Z"
    }
   },
   "outputs": [],
   "source": [
    "# idx = 356\n",
    "print(image[..., 0].min(), image[..., 0].max(), image[...,0].mean())\n",
    "# plt.hist()\n",
    "# print(image.min(), image.max(), image.mean())\n",
    "# grey_image = image[..., 0]\n",
    "augmented = AUG(image=image, mask=image)\n",
    "input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "## Get gaussian\n",
    "# random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n",
    "# gauss = random_state.normal(0, 0.1, input.shape)\n",
    "# input = input + gauss\n",
    "# gauss\n",
    "\n",
    "# print(\"Input\", input.shape, input.min(), input.max())\n",
    "# print(\"Target\", target.shape, target.min(), target.max())\n",
    "\n",
    "# augmented = NORM_TO_TENSOR(image=input, mask=target)\n",
    "# input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "print(\"Input\", input.shape, input.min(), input.max())\n",
    "print(\"Target\", target.shape, target.min(), target.max())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(torch.cat([input[0], target[0]], dim=1)) #.permute(1, 2, 0))\n",
    "# plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0))\n",
    "\n",
    "torch.sum(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:15.660432Z",
     "start_time": "2020-04-26T10:46:15.622703Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.mean(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T10:12:01.132632Z",
     "start_time": "2020-04-25T10:12:01.115628Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow((medicaldecathlon[idx][0][..., 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:31:31.339877Z",
     "start_time": "2020-04-24T21:31:31.334100Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(256, 256)\n",
    "a = a[:,:,np.newaxis].repeat(3, axis=2)\n",
    "# a.repeat(3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:04:44.034846Z",
     "start_time": "2020-04-24T21:04:43.900165Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:00:05.101620Z",
     "start_time": "2020-05-29T16:00:05.098635Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:18:18.394762Z",
     "start_time": "2020-05-29T16:18:10.979470Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = [\"div2k\", \"bsds100\", \"set5\"]\n",
    "transform = get_aug(aug_type=\"val\", task=\"deblur\", size=128)\n",
    "\n",
    "for dataset in datasets:\n",
    "#     train_loader = get_dataloader(dataset, transform, train=True, batch_size=64)\n",
    "    \n",
    "#     for batch in train_loader:\n",
    "#         input, output = batch\n",
    "#         print(input.shape, output.shape)\n",
    "        \n",
    "    \n",
    "    val_loader = get_dataloader(dataset, transform, train=False)\n",
    "    for batch in val_loader:\n",
    "        input, output = batch\n",
    "        print(input.shape, output.shape)\n",
    "    \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:14:07.954107Z",
     "start_time": "2020-05-29T16:14:07.942725Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = BSDS100(\"datasets/BSDS100\", train=True, transform=transform)\n",
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:52:16.029947Z",
     "start_time": "2020-04-14T18:52:16.024488Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "image_metrics = [\"kid\", {}, \"ssim\", {}, ]\n",
    "\n",
    "METRIC_FROM_NAME = {\n",
    "    \"ssim\" : pm.SSIMLoss,\n",
    "    \"ms-ssim\" : pm.MultiScaleSSIMLoss,\n",
    "    \"msid\" : pm.MSID,\n",
    "    \"fid\" : pm.FID,\n",
    "    \"kid\" : pm.KID,\n",
    "#     \"content\" : ContentLoss,\n",
    "#     \"style\" : StyleLoss,\n",
    "    \"tv\" : pm.TVLoss,\n",
    "}\n",
    "\n",
    "\n",
    "# for metric in image_metrics:\n",
    "image_metrics = [METRIC_FROM_NAME[metric](**kwargs) for metric, kwargs in zip(image_metrics[::2], image_metrics[1::2])]\n",
    "# list(zip(image_metrics[::2], image_metrics[1::2]))\n",
    "image_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:29:09.787795Z",
     "start_time": "2020-04-14T19:29:09.783997Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "output = OrderedDict({\n",
    "    'loss': 1,\n",
    "#     'mse': mse,\n",
    "#     'psnr': psnr,\n",
    "#     'ssim': ssim_score,\n",
    "#     # 'val_ms_ssim': ms_ssim_score,\n",
    "#     'input_features': input_features,\n",
    "#     'target_features': target_features\n",
    "})\n",
    "output[\"test\"] = 3\n",
    "output[\"foo\"] = \"bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:08:23.644634Z",
     "start_time": "2020-04-26T15:08:23.508619Z"
    }
   },
   "outputs": [],
   "source": [
    "images = torch.rand((512, 2, 32, 32))\n",
    "target = torch.rand((512, 2, 64, 64))\n",
    "print(f\"Before interpolation: images {images.shape}\")\n",
    "images = F.interpolate(images, size=target.shape[-2:], mode=\"bilinear\")\n",
    "print(f\"After interpolation: images {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import b\n",
    "# \n",
    "iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T20:36:25.754041Z",
     "start_time": "2020-04-14T20:36:25.750728Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [\"foo\", \"bar\"]\n",
    "if \"foo\" in a:\n",
    "    print(a.index(\"foo\"))\n",
    "# a.append(\"loss\")\n",
    "# print(a)\n",
    "# a.pop()\n",
    "# a\n",
    "# for i in \"etc\", a:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:36.482862Z",
     "start_time": "2020-04-14T16:45:34.767973Z"
    }
   },
   "outputs": [],
   "source": [
    "features = make_layers(\n",
    "    [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"], \n",
    "    batch_norm=True\n",
    ")\n",
    "vgg_16 = VGG(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:37.757692Z",
     "start_time": "2020-04-14T16:45:37.751504Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:10.728420Z",
     "start_time": "2020-04-14T16:47:09.880830Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(10, 3, 224, 224)\n",
    "layers = None\n",
    "if layers is None:\n",
    "    layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "features = []\n",
    "for name, module in vgg_16.features._modules.items():\n",
    "    x = module(x)\n",
    "    if name in layers:\n",
    "        features.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:17.479314Z",
     "start_time": "2020-04-14T16:47:17.414244Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:30:14.494629Z",
     "start_time": "2020-04-14T14:30:14.490716Z"
    }
   },
   "outputs": [],
   "source": [
    "# from albumentations import ImageOnlyTransform\n",
    "albu.ImageCompression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:20.318480Z",
     "start_time": "2020-04-14T13:26:20.314695Z"
    }
   },
   "outputs": [],
   "source": [
    "target.permute(2, 0, 1).shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:30.322997Z",
     "start_time": "2020-04-14T13:26:30.167211Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(target.permute(2, 0, 1).transpose(0, 2).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:18:02.700784Z",
     "start_time": "2020-04-14T11:18:02.475697Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TinyImageNetDataset(train=True, transform=None, target_transform=None)\n",
    "input, target = dataset[6]\n",
    "# input.shape\n",
    "# plt.imshow(input / 255.)\n",
    "(input / 255.).min(), (input / 255.).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:09:29.108056Z",
     "start_time": "2020-04-14T11:09:29.104053Z"
    }
   },
   "outputs": [],
   "source": [
    "input.transpose(0, 2).transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:02:42.360140Z",
     "start_time": "2020-04-14T11:02:42.355048Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T10:52:13.348128Z",
     "start_time": "2020-04-09T10:52:13.337949Z"
    }
   },
   "outputs": [],
   "source": [
    "# from src.augmentations import get_aug\n",
    "# from src.datasets import MNIST, CIFAR10, CIFAR100\n",
    "from ..src.datasets\n",
    "# from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:11.714091Z",
     "start_time": "2020-04-08T14:27:11.660050Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type=\"light\", size=32)\n",
    "target_transform = get_aug(aug_type=\"val\", size=32)\n",
    "# target_transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:12.035620Z",
     "start_time": "2020-04-08T14:27:11.981391Z"
    }
   },
   "outputs": [],
   "source": [
    "transform, target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.384001Z",
     "start_time": "2020-04-08T14:27:12.625838Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root='../datasets', train=True, transform=transform, target_transform=target_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# valset = CIFAR10(root='../datasets', train=False, transform=transform, target_transform=target_transform)\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=16,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.589831Z",
     "start_time": "2020-04-08T14:27:13.534077Z"
    }
   },
   "outputs": [],
   "source": [
    "image, target = trainset[0]\n",
    "print(image.shape, target.shape)\n",
    "print(image.max(), image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:14.716283Z",
     "start_time": "2020-04-08T14:27:14.483971Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "input, target = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:16.860006Z",
     "start_time": "2020-04-08T14:27:16.789254Z"
    }
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:04.837084Z",
     "start_time": "2020-04-08T15:09:04.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "model.fc = Identity()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:07.391805Z",
     "start_time": "2020-04-08T15:09:07.348692Z"
    }
   },
   "outputs": [],
   "source": [
    "mock = torch.rand((3, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:08.273261Z",
     "start_time": "2020-04-08T15:09:08.185666Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model(mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:15:02.858962Z",
     "start_time": "2020-04-08T15:15:02.810410Z"
    }
   },
   "outputs": [],
   "source": [
    "all_input_features = [result.detach() for _ in range(4)]\n",
    "print(len(all_input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:16:17.105471Z",
     "start_time": "2020-04-08T15:16:17.056511Z"
    }
   },
   "outputs": [],
   "source": [
    "input_features = torch.cat(all_input_features, dim=0)\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:54:58.027998Z",
     "start_time": "2020-04-08T14:54:57.979447Z"
    }
   },
   "outputs": [],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:29:33.595967Z",
     "start_time": "2020-04-24T17:29:33.564780Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:11:48.608920Z",
     "start_time": "2020-04-24T19:11:48.604678Z"
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"datasets/decathlon/colon.h5\"\n",
    "with h5py.File(datapath, \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "#     data_val = f['imgs_validation'][::10]\n",
    "#     data_test = f['imgs_testing'][::10]\n",
    "#     data = np.concatenate((data_val, data_test))\n",
    "#     print(data.shape)\n",
    "#     print(len(data))\n",
    "    \n",
    "\n",
    "# hf = \n",
    "# data_numpy = np.zeros(hf['imgs_train'].shape, dtype=numpy_type)\n",
    "# # hf['dataset_name'].read_direct(n1)\n",
    "# # hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:17:10.798726Z",
     "start_time": "2020-04-24T19:17:10.390446Z"
    }
   },
   "outputs": [],
   "source": [
    "data = hf['imgs_train'][5000]\n",
    "mask = hf['msks_train'][5000]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data.squeeze(), cmap='gray')\n",
    "# plt.imshow(mask.squeeze(), alpha=0.1)\n",
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:13:31.176828Z",
     "start_time": "2020-04-24T19:13:30.998749Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:43:43.812272Z",
     "start_time": "2020-04-24T17:43:43.677479Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:31:12.884247Z",
     "start_time": "2020-04-24T17:31:12.881134Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:06:57.805822Z",
     "start_time": "2020-05-13T16:06:57.802189Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3                                                                                                                 \n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import photosynthesis_metrics as pm    \n",
    "from photosynthesis_metrics import IS\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:07:03.862241Z",
     "start_time": "2020-05-13T16:07:03.179834Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "cifar10 = CIFAR10(root=\"/home/zakirov/repoz/metrics-comparison/datasets\", train=True, transform=aug)\n",
    "cifar10\n",
    "loader = torch.utils.data.DataLoader(cifar10, batch_size=100, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:16:23.449739Z",
     "start_time": "2020-05-13T15:16:20.585333Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = pm.feature_extractors.fid_inception.InceptionV3(\n",
    "#     resize_input=True, \n",
    "#     normalize_input=True,\n",
    "#     requires_grad=False,\n",
    "#     use_fid_inception=False,\n",
    "# )\n",
    "model = inception_v3(pretrained=True, transform_input=False).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:43:51.665311Z",
     "start_time": "2020-05-19T20:43:51.663129Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T18:37:01.177723Z",
     "start_time": "2020-05-18T18:37:01.119361Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = pm.GS(num_iters=20)\n",
    "f1 = torch.rand(1000, 20)\n",
    "f2 = torch.rand(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T21:22:15.649984Z",
     "start_time": "2020-05-19T21:22:15.644455Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read 2 images used in acticle and compute score for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:56:58.494183Z",
     "start_time": "2020-05-20T13:56:58.093684Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type='light', task='deblur', size=64)\n",
    "\n",
    "loader = get_dataloader(\n",
    "    datasets=['tinyimagenet'],\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "for batch in loader:\n",
    "    break\n",
    "    \n",
    "lr = batch[0][1]\n",
    "hr = batch[1][1]\n",
    "print(lr.min(), lr.max())\n",
    "plt.imshow((lr.permute(1,2,0) + 1) * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:26:16.320593Z",
     "start_time": "2020-07-01T22:26:16.307301Z"
    }
   },
   "outputs": [],
   "source": [
    "def fsim(x: torch.Tensor, y: torch.Tensor, reduction: str = 'mean',\n",
    "         data_range: Union[int, float] = 1.0, chromatic: bool = True,\n",
    "         scales: int = 4, orientations: int = 4, min_length: int = 6,\n",
    "         mult: int = 2, sigma_f: float = 0.55, delta_theta: float = 1.2,\n",
    "         k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Feature Similarity Index Measure for a batch of images.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        x: Batch of predicted images with shape (batch_size x channels x H x W)\n",
    "        y: Batch of target images with shape  (batch_size x channels x H x W)\n",
    "        data_range: Value range of input images (usually 1.0 or 255). Default: 1.0\n",
    "        chromatic: Flag to compute FSIMc, which also takes into account chromatic components\n",
    "        scales: Number of wavelets used for computation of phase congruensy maps\n",
    "        orientations: Number of filter orientations used for computation of phase congruensy maps\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian describing the log Gabor filter's\n",
    "            transfer function in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations and the standard deviation\n",
    "            of the angular Gaussian function used to construct filters in the frequency plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean at which we set the noise\n",
    "            threshold  point, below which phase congruency values get penalized.\n",
    "        \n",
    "    Returns:\n",
    "        FSIM: Index of similarity betwen two images. Usually in [0, 1] interval.\n",
    "            Can be bigger than 1 for predicted images with higher contrast than original one.\n",
    "    Note:\n",
    "        This implementation is based on original authors MATLAB code.\n",
    "        https://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/FSIM.htm\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    _validate_input(input_tensors=(x, y), allow_5d=False)\n",
    "    x, y = _adjust_dimensions(input_tensors=(x, y))\n",
    "    \n",
    "    # Scale to [0., 1.] range\n",
    "    x = x / float(data_range)\n",
    "    y = y / float(data_range)\n",
    "    \n",
    "    # Rescale to [0, 255] range, because all constant are calculated for this factor\n",
    "    x = x * 255\n",
    "    y = y * 255\n",
    "    \n",
    "    # Apply average pooling\n",
    "    kernel_size = max(1, round(min(x.shape[-2:]) / 256))\n",
    "    x = torch.nn.functional.avg_pool2d(x, kernel_size, stride=2)\n",
    "    y = torch.nn.functional.avg_pool2d(y, kernel_size, stride=2)\n",
    "        \n",
    "    num_channels = x.size(1)\n",
    "\n",
    "    # Convert RGB to YIQ color space https://en.wikipedia.org/wiki/YIQ\n",
    "    if num_channels == 3:\n",
    "        yiq_weights = torch.tensor([\n",
    "            [0.299, 0.587, 0.114],\n",
    "            [0.5959, -0.2746, -0.3213],\n",
    "            [0.2115, -0.5227, 0.3112]]).t().to(x)\n",
    "        x = torch.matmul(x.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        y = torch.matmul(y.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_lum = x[:, : 1, :, :]\n",
    "        y_lum = y[:, : 1, :, :]\n",
    "        \n",
    "        x_i = x[:, 1, :, :]\n",
    "        y_i = y[:, 1, :, :]\n",
    "        x_q = x[:, 2, :, :]\n",
    "        y_q = y[:, 2, :, :]\n",
    "\n",
    "    else:\n",
    "        x_lum = x\n",
    "        y_lum = y\n",
    "\n",
    "    # Compute phase congruency maps\n",
    "    pc_x = _phase_congruency(\n",
    "        x_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    pc_y = _phase_congruency(\n",
    "        y_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    \n",
    "    # Gradient maps\n",
    "    kernels = torch.stack([scharr_filter(), scharr_filter().transpose(-1, -2)])\n",
    "    grad_map_x = gradient_map(x_lum, kernels)\n",
    "    grad_map_y = gradient_map(y_lum, kernels)\n",
    "    \n",
    "    # Constants from paper\n",
    "    T1, T2, T3, T4, lmbda = 0.85, 160, 200, 200, 0.03\n",
    "    \n",
    "    # Compute FSIM\n",
    "    PC = similarity_map(pc_x, pc_y, T1)\n",
    "    GM = similarity_map(grad_map_x, grad_map_y, T2)\n",
    "    pc_max = torch.where(pc_x > pc_y, pc_x, pc_y)\n",
    "    score = GM * PC * pc_max\n",
    "    \n",
    "    if chromatic:\n",
    "        S_I = similarity_map(x_i, y_i, T3)\n",
    "        S_Q = similarity_map(x_q, y_q, T4)\n",
    "        score = score * torch.abs(S_I * S_Q) ** lmbda\n",
    "        # Complex gradients will work in PyTorch 1.6.0\n",
    "        # score = score * torch.real((S_I * S_Q).to(torch.complex64) ** lmbda)\n",
    "\n",
    "    result = score.sum(dim=[1, 2, 3]) / pc_max.sum(dim=[1, 2])\n",
    "    \n",
    "    if reduction == 'none':\n",
    "        return result\n",
    "\n",
    "    return {'mean': result.mean,\n",
    "            'sum': result.sum\n",
    "            }[reduction](dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _phase_congruency(x: torch.Tensor, scales: int = 4, orientations: int = 4,\n",
    "                      min_length: int = 6, mult: int = 2, sigma_f: float = 0.55,\n",
    "                      delta_theta: float = 1.2, k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Phase Congruence for a batch of greyscale images\n",
    "\n",
    "    Args:\n",
    "        x: Tensor with shape Bx1xHxW\n",
    "        levels: Number of wavelet scales\n",
    "        orientations: Number of filter orientations\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian\n",
    "            describing the log Gabor filter's transfer function\n",
    "            in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations\n",
    "            and the standard deviation of the angular Gaussian function\n",
    "            used to construct filters in the freq. plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean\n",
    "            at which we set the noise threshold point, below which phase\n",
    "            congruency values get penalized.\n",
    "    Returns:\n",
    "        PCmap: Tensor with shape BxHxW\n",
    "\n",
    "    \"\"\"\n",
    "    EPS = 1e-4\n",
    "\n",
    "    B, _, H, W = x.shape\n",
    "\n",
    "    # Fourier transform\n",
    "    imagefft = torch.rfft(x, 2, onesided=False)\n",
    "\n",
    "    filters = _construct_filters(x, scales, orientations, min_length, mult, sigma_f, delta_theta, k)\n",
    "\n",
    "    # Note rescaling to match power record ifft2 of filter\n",
    "    filters_ifft = torch.ifft(torch.stack([filters, torch.zeros_like(filters)], dim=-1), 2)[..., 0] * math.sqrt(H * W)\n",
    "    \n",
    "    # Convolve image with even and odd filters\n",
    "    E0 = torch.ifft(imagefft * filters.unsqueeze(-1), 2).view(B, orientations, scales, H, W, 2)\n",
    "\n",
    "    # Amplitude of even & odd filter response. An = sqrt(real^2 + imag^2)\n",
    "    an = torch.sqrt(torch.sum(E0 ** 2, dim=-1))\n",
    "\n",
    "    # Take filter at scale 0 and sum spatially\n",
    "    # Record mean squared filter value at smallest scale.\n",
    "    # This is used for noise estimation.\n",
    "    em_n = (filters.view(1, orientations, scales, H, W)[:, :, :1, ...] ** 2).sum(dim=[-2, -1], keepdims=True)\n",
    "\n",
    "    # Sum of even filter convolution results.\n",
    "    sum_e = E0[..., 0].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Sum of odd filter convolution results.\n",
    "    sum_o = E0[..., 1].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Get weighted mean filter response vector, this gives the weighted mean phase angle.\n",
    "    x_energy = torch.sqrt(sum_e ** 2 + sum_o ** 2) + EPS\n",
    "\n",
    "    mean_e = sum_e / x_energy\n",
    "    mean_o = sum_o / x_energy\n",
    "\n",
    "    # Now calculate An(cos(phase_deviation) - | sin(phase_deviation)) | by\n",
    "    # using dot and cross products between the weighted mean filter response\n",
    "    # vector and the individual filter response vectors at each scale.\n",
    "    # This quantity is phase congruency multiplied by An, which we call energy.\n",
    "\n",
    "    # Extract even and odd convolution results.\n",
    "    E = E0[..., 0]\n",
    "    O = E0[..., 1]\n",
    "\n",
    "    energy = (E * mean_e + O * mean_o - torch.abs(E * mean_o - O * mean_e)).sum(dim=2, keepdim=True)\n",
    "    \n",
    "    # Compensate for noise\n",
    "    # We estimate the noise power from the energy squared response at the\n",
    "    # smallest scale.  If the noise is Gaussian the energy squared will have a\n",
    "    # Chi-squared 2DOF pdf.  We calculate the median energy squared response\n",
    "    # as this is a robust statistic.  From this we estimate the mean.\n",
    "    # The estimate of noise power is obtained by dividing the mean squared\n",
    "    # energy value by the mean squared filter value\n",
    "    \n",
    "    abs_e0 = torch.sqrt(torch.sum(E0[:, :, :1, ...] ** 2, dim=-1)).reshape(B, orientations, 1, 1, H * W)\n",
    "    median_e2n = torch.median(abs_e0 ** 2, dim=-1, keepdims=True).values\n",
    "\n",
    "    mean_e2n = - median_e2n / math.log(0.5)\n",
    "\n",
    "    # Estimate of noise power.\n",
    "    noisePower = mean_e2n / em_n\n",
    "    \n",
    "    # Now estimate the total energy^2 due to noise\n",
    "    # Estimate for sum(An^2) + sum(Ai.*Aj.*(cphi.*cphj + sphi.*sphj))\n",
    "    filters_ifft = filters_ifft.view(1, orientations, scales, H, W)\n",
    "    \n",
    "    sum_an2 = torch.sum(filters_ifft ** 2, dim=-3, keepdim=True)\n",
    "    \n",
    "    sum_ai_aj = torch.zeros(B, orientations, 1, H, W).to(x)\n",
    "    for s in range(scales - 1):\n",
    "        sum_ai_aj = sum_ai_aj + (filters_ifft[:, :, s: s + 1] * filters_ifft[:, :, s + 1:]).sum(dim=-3, keepdim=True)\n",
    "            \n",
    "    sum_an2 = torch.sum(sum_an2, dim=[-1, -2], keepdim=True)\n",
    "    sum_ai_aj = torch.sum(sum_ai_aj, dim=[-1, -2], keepdim=True)\n",
    "\n",
    "    noise_energy2 = 2 * noisePower * sum_an2 + 4 * noisePower * sum_ai_aj\n",
    "\n",
    "    # Rayleigh parameter\n",
    "    tau = torch.sqrt(noise_energy2 / 2)\n",
    "\n",
    "    # Expected value of noise energy\n",
    "    noise_energy = tau * math.sqrt(math.pi / 2)\n",
    "    moise_energy_sigma = torch.sqrt((2 - math.pi / 2) * tau ** 2)\n",
    "\n",
    "    # Noise threshold\n",
    "    T = noise_energy + k * moise_energy_sigma\n",
    "\n",
    "    # The estimated noise effect calculated above is only valid for the PC_1 measure.\n",
    "    # The PC_2 measure does not lend itself readily to the same analysis.  However\n",
    "    # empirically it seems that the noise effect is overestimated roughly by a factor\n",
    "    # of 1.7 for the filter parameters used here.\n",
    "\n",
    "    # Empirical rescaling of the estimated noise effect to suit the PC_2 phase congruency measure\n",
    "    T = T / 1.7\n",
    "\n",
    "    # Apply noise threshold\n",
    "    energy = torch.max(energy - T, torch.zeros_like(T))\n",
    "\n",
    "    energy_all = energy.sum(dim=[1, 2])\n",
    "    \n",
    "    sum_an = an.sum(dim=2, keepdims=True)\n",
    "    an_all = sum_an.sum(dim=[1, 2])\n",
    "    # an_all = an.sum(dim=[1, 2])\n",
    "    \n",
    "    result_pc = energy_all / an_all\n",
    "    return result_pc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
