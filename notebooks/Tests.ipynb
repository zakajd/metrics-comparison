{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T15:40:37.231851Z",
     "start_time": "2020-07-01T15:40:37.111752Z"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:36:45.894064Z",
     "start_time": "2020-07-10T13:36:44.764446Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random \n",
    "import functools\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import albumentations as albu\n",
    "import albumentations.pytorch as albu_pt\n",
    "import piq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:36:46.098869Z",
     "start_time": "2020-07-10T13:36:45.895973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:100% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 10 2020 \n",
      "\n",
      "CPython 3.6.9\n",
      "IPython 7.8.0\n",
      "\n",
      "numpy 1.17.0\n",
      "torch 1.6.0.dev20200610\n",
      "albumentations 0.4.5\n",
      "photosynthesis_metrics not installed\n",
      "\n",
      "compiler   : GCC 8.4.0\n",
      "system     : Linux\n",
      "release    : 4.15.0-108-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:100% !important;}</style>\"))\n",
    "\n",
    "# Fix to be able to import python modules inside a notebook\n",
    "os.chdir('..')\n",
    "\n",
    "# Useful extensions\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,torch,albumentations,photosynthesis_metrics\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Nice plot formating\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T13:36:46.227751Z",
     "start_time": "2020-07-10T13:36:46.105078Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "configs  logs\t   models     README.md  requirements.txt  tests\r\n",
      "data\t Makefile  notebooks  reports\t src\t\t   WORKPLAN.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T14:08:33.827365Z",
     "start_time": "2020-07-10T14:08:33.768617Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of Content loss, Style loss and LPIPS metric\n",
    "References:\n",
    "    .. [1] Gatys, Leon and Ecker, Alexander and Bethge, Matthias\n",
    "    (2016). A Neural Algorithm of Artistic Style}\n",
    "    Association for Research in Vision and Ophthalmology (ARVO)\n",
    "    https://arxiv.org/abs/1508.06576\n",
    "\n",
    "    .. [2] Zhang, Richard and Isola, Phillip and Efros, et al.\n",
    "    (2018) The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\n",
    "    2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
    "    https://arxiv.org/abs/1801.03924\n",
    "\"\"\"\n",
    "from typing import List, Union, Callable, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torchvision.models import vgg16, vgg19\n",
    "\n",
    "from piq.utils.common import _validate_input, _adjust_dimensions\n",
    "from piq.utils.functional import similarity_map\n",
    "from piq.utils.layers import L2Pool2d\n",
    "\n",
    "\n",
    "# Map VGG names to corresponding number in torchvision layer\n",
    "VGG16_LAYERS = {\n",
    "    \"conv1_1\": '0', \"relu1_1\": '1',\n",
    "    \"conv1_2\": '2', \"relu1_2\": '3',\n",
    "    \"pool1\": '4',\n",
    "    \"conv2_1\": '5', \"relu2_1\": '6',\n",
    "    \"conv2_2\": '7', \"relu2_2\": '8',\n",
    "    \"pool2\": '9',\n",
    "    \"conv3_1\": '10', \"relu3_1\": '11',\n",
    "    \"conv3_2\": '12', \"relu3_2\": '13',\n",
    "    \"conv3_3\": '14', \"relu3_3\": '15',\n",
    "    \"pool3\": '16',\n",
    "    \"conv4_1\": '17', \"relu4_1\": '18',\n",
    "    \"conv4_2\": '19', \"relu4_2\": '20',\n",
    "    \"conv4_3\": '21', \"relu4_3\": '22',\n",
    "    \"pool4\": '23',\n",
    "    \"conv5_1\": '24', \"relu5_1\": '25',\n",
    "    \"conv5_2\": '26', \"relu5_2\": '27',\n",
    "    \"conv5_3\": '28', \"relu5_3\": '29',\n",
    "    \"pool5\": '30',\n",
    "}\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Constant used in feature normalization to avoid zero division\n",
    "EPS = 1e-10\n",
    "\n",
    "\n",
    "class ContentLoss(_Loss):\n",
    "    r\"\"\"Creates Content loss that can be used for image style transfer of as a measure in\n",
    "    image to image tasks.\n",
    "    Uses pretrained VGG models from torchvision. Normalizes features before summation.\n",
    "    Expects input to be in range [0, 1] or normalized with ImageNet statistics into range [-1, 1]\n",
    "\n",
    "    Args:\n",
    "        feature_extractor: Model to extract features or model name in {`vgg16`, `vgg19`}.\n",
    "        layers: List of strings with layer names. Default: [`relu3_3`]\n",
    "        weights: List of float weight to balance different layers\n",
    "        replace_pooling: Flag to replace MaxPooling layer with AveragePooling. See [1] for details.\n",
    "        distance: Method to compute distance between features. One of {`mse`, `mae`}.\n",
    "        reduction: Reduction over samples in batch: \"mean\"|\"sum\"|\"none\"\n",
    "        mean: List of float values used for data standartization. Default: ImageNet mean.\n",
    "            If there is no need to normalize data, use [0., 0., 0.].\n",
    "        std: List of float values used for data standartization. Default: ImageNet std.\n",
    "            If there is no need to normalize data, use [1., 1., 1.].\n",
    "        normalize_features: If true, unit-normalize each feature in channel dimension before scaling\n",
    "            and computing distance. See [2] for details.\n",
    "\n",
    "    References:\n",
    "        .. [1] Gatys, Leon and Ecker, Alexander and Bethge, Matthias\n",
    "        (2016). A Neural Algorithm of Artistic Style}\n",
    "        Association for Research in Vision and Ophthalmology (ARVO)\n",
    "        https://arxiv.org/abs/1508.06576\n",
    "\n",
    "        .. [2] Zhang, Richard and Isola, Phillip and Efros, et al.\n",
    "        (2018) The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\n",
    "        2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n",
    "        https://arxiv.org/abs/1801.03924\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_extractor: Union[str, Callable] = \"vgg16\", layers: Tuple[str] = (\"relu3_3\", ),\n",
    "                 weights: List[Union[float, torch.Tensor]] = [1.], replace_pooling: bool = False,\n",
    "                 distance: str = \"mse\", reduction: str = \"mean\", mean: List[float] = IMAGENET_MEAN,\n",
    "                 std: List[float] = IMAGENET_STD, normalize_features: bool = False) -> None:\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if callable(feature_extractor):\n",
    "            self.model = feature_extractor\n",
    "            self.layers = layers\n",
    "        else:\n",
    "            if feature_extractor == \"vgg16\":\n",
    "                self.model = vgg16(pretrained=True, progress=False).features\n",
    "                self.layers = [VGG16_LAYERS[l] for l in layers]\n",
    "            elif feature_extractor == \"vgg19\":\n",
    "                self.model = vgg19(pretrained=True, progress=False).features\n",
    "                self.layers = [VGG19_LAYERS[l] for l in layers]\n",
    "            else:\n",
    "                raise ValueError(\"Unknown feature extractor\")\n",
    "\n",
    "        if replace_pooling:\n",
    "            self.model = self.replace_pooling(self.model)\n",
    "\n",
    "        # Disable gradients\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "\n",
    "        self.distance = {\n",
    "            \"mse\": nn.MSELoss,\n",
    "            \"mae\": nn.L1Loss,\n",
    "        }[distance](reduction='none')\n",
    "\n",
    "        self.weights = [torch.tensor(w) for w in weights]\n",
    "        mean = torch.tensor(mean)\n",
    "        std = torch.tensor(std)\n",
    "        self.mean = mean.view(1, -1, 1, 1)\n",
    "        self.std = std.view(1, -1, 1, 1)\n",
    "        \n",
    "        self.normalize_features = normalize_features\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, prediction: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Computation of Content loss between feature representations of prediction and target tensors.\n",
    "\n",
    "        Args:\n",
    "            prediction: Tensor of prediction of the network.\n",
    "            target: Reference tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        _validate_input(input_tensors=(prediction, target), allow_5d=False)\n",
    "        prediction, target = _adjust_dimensions(input_tensors=(prediction, target))\n",
    "\n",
    "        self.model.to(prediction)\n",
    "        prediction_features = self.get_features(prediction)\n",
    "        target_features = self.get_features(target)\n",
    "\n",
    "        distances = self.compute_distance(prediction_features, target_features)\n",
    "\n",
    "        # Scale distances, then average in spatial dimensions, then stack and sum in channels dimension\n",
    "        loss = torch.cat([(d * w.to(d)).mean(dim=[2, 3]) for d, w in zip(distances, self.weights)], dim=1).sum(dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return loss\n",
    "\n",
    "        return {'mean': loss.mean,\n",
    "                'sum': loss.sum\n",
    "                }[self.reduction](dim=0)\n",
    "\n",
    "    def compute_distance(self, prediction_features: torch.Tensor, target_features: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Take L2 or L1 distance between feature maps\"\"\"\n",
    "        return [self.distance(x, y) for x, y in zip(prediction_features, target_features)]\n",
    "\n",
    "    def get_features(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor with shape (N, C, H, W)\n",
    "        \n",
    "        Returns:\n",
    "            features: List of features extracted from intermediate layers\n",
    "        \"\"\"\n",
    "        # Normalize input\n",
    "        x = (x - self.mean.to(x)) / self.std.to(x)\n",
    "\n",
    "        features = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.layers:\n",
    "                features.append(self.normalize(x) if self.normalize_features else x)\n",
    "        return features\n",
    "\n",
    "    def normalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Normalize feature maps in channel direction to unit length.\n",
    "        Args:\n",
    "            x: Tensor with shape (N, C, H, W)\n",
    "        Returns:\n",
    "            x_norm: Normalized input\n",
    "        \"\"\"\n",
    "        norm_factor = torch.sqrt(torch.sum(x ** 2, dim=1, keepdim=True))\n",
    "        return x / (norm_factor + EPS)\n",
    "\n",
    "    def replace_pooling(self, module: torch.nn.Module) -> torch.nn.Module:\n",
    "        r\"\"\"Turn All MaxPool layers into AveragePool\"\"\"\n",
    "        module_output = module\n",
    "        if isinstance(module, torch.nn.MaxPool2d):\n",
    "            module_output = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "            \n",
    "        for name, child in module.named_children():\n",
    "            module_output.add_module(name, self.replace_pooling(child))\n",
    "        return module_output\n",
    "\n",
    "\n",
    "class DISTS(ContentLoss):\n",
    "    r\"\"\"Deep Image Structure and Texture Similarity metric.\n",
    "    Expects input to be in range [0, 1] or normalized with ImageNet statistics into range [-1, 1]\n",
    "\n",
    "    Args:\n",
    "        layers: List of strings with layer names. Default: [`relu3_3`]\n",
    "        reduction: Reduction over samples in batch: \"mean\"|\"sum\"|\"none\"\n",
    "        mean: List of float values used for data standartization. Default: ImageNet mean.\n",
    "            If there is no need to normalize data, use [0., 0., 0.].\n",
    "        std: List of float values used for data standartization. Default: ImageNet std.\n",
    "            If there is no need to normalize data, use [1., 1., 1.].\n",
    "\n",
    "    References:\n",
    "        .. [1] Keyan Ding, Kede Ma, Shiqi Wang, Eero P. Simoncelli\n",
    "        (2020). Image Quality Assessment: Unifying Structure and Texture Similarity.\n",
    "        https://arxiv.org/abs/2004.07728\n",
    "        .. [2] https://github.com/dingkeyan93/DISTS\n",
    "    \"\"\"\n",
    "    _weights_url = \"https://github.com/photosynthesis-team/piq/releases/download/v0.4.1/dists_weights.pt\"\n",
    "\n",
    "    def __init__(self, reduction: str = \"mean\", mean: List[float] = IMAGENET_MEAN,\n",
    "                 std: List[float] = IMAGENET_STD) -> None:\n",
    "        dists_layers = ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3']\n",
    "        channels = [3, 64, 128, 256, 512, 512]\n",
    "\n",
    "        weights = torch.hub.load_state_dict_from_url(self._weights_url, progress=False)\n",
    "        dists_weights = list(torch.split(weights['alpha'], channels, dim=1))\n",
    "        dists_weights.extend(torch.split(weights['beta'], channels, dim=1))\n",
    "\n",
    "        super().__init__(\"vgg16\", layers=dists_layers, weights=dists_weights,\n",
    "                         replace_pooling=True, reduction=reduction,\n",
    "                         mean=mean, std=std, normalize_features=False)\n",
    "\n",
    "    def forward(self, prediction: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        loss = super().forward(prediction, target)\n",
    "        return 1 - loss\n",
    "\n",
    "    def compute_distance(self, prediction_features: torch.Tensor, target_features: torch.Tensor) -> List[torch.Tensor]:\n",
    "        r\"\"\"Compute structure similarity between feature maps\"\"\"\n",
    "        structure_distance, texture_distance = [], []\n",
    "        # Small constant for numerical stability\n",
    "        EPS = 1e-5\n",
    "\n",
    "        for x, y in zip(prediction_features, target_features):\n",
    "            x_mean = x.mean([2, 3], keepdim=True)\n",
    "            y_mean = y.mean([2, 3], keepdim=True)\n",
    "            structure_distance.append(similarity_map(x_mean, y_mean, constant=EPS))\n",
    "\n",
    "            x_var = ((x - x_mean) ** 2).mean([2, 3], keepdim=True)\n",
    "            y_var = ((y - y_mean) ** 2).mean([2, 3], keepdim=True)\n",
    "            xy_cov = (x * y).mean([2, 3], keepdim=True) - x_mean * y_mean\n",
    "            texture_distance.append((2 * xy_cov + EPS) / (x_var + y_var + EPS))\n",
    "\n",
    "        return structure_distance + texture_distance\n",
    "\n",
    "    def get_features(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
    "        features = super().get_features(x)\n",
    "\n",
    "        # Add input tensor as an additional feature\n",
    "        features.insert(0, x)\n",
    "        return features\n",
    "\n",
    "    def replace_pooling(self, module: torch.nn.Module) -> torch.nn.Module:\n",
    "        r\"\"\"Turn All MaxPool layers into L2Pool\"\"\"\n",
    "        module_output = module\n",
    "        if isinstance(module, torch.nn.MaxPool2d):\n",
    "            module_output = L2Pool2d(kernel_size=3, stride=2, padding=1)\n",
    "            \n",
    "        for name, child in module.named_children():\n",
    "            module_output.add_module(name, self.replace_pooling(child))\n",
    "        return module_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T14:08:34.997991Z",
     "start_time": "2020-07-10T14:08:34.972848Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os,sys\n",
    "import torch\n",
    "from torchvision import models,transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class L2pooling(nn.Module):\n",
    "    def __init__(self, filter_size=5, stride=2, channels=None, pad_off=0):\n",
    "        super(L2pooling, self).__init__()\n",
    "        self.padding = (filter_size - 2 )//2\n",
    "        self.stride = stride\n",
    "        self.channels = channels\n",
    "        a = np.hanning(filter_size)[1:-1]\n",
    "        g = torch.Tensor(a[:,None]*a[None,:])\n",
    "        g = g/torch.sum(g)\n",
    "        self.register_buffer('filter', g[None,None,:,:].repeat((self.channels,1,1,1)))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input**2\n",
    "        out = F.conv2d(input, self.filter, stride=self.stride, padding=self.padding, groups=input.shape[1])\n",
    "        return (out+1e-12).sqrt()\n",
    "\n",
    "class DISTS_orig(torch.nn.Module):\n",
    "    def __init__(self, load_weights=True):\n",
    "        super(DISTS_orig, self).__init__()\n",
    "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
    "        self.stage1 = torch.nn.Sequential()\n",
    "        self.stage2 = torch.nn.Sequential()\n",
    "        self.stage3 = torch.nn.Sequential()\n",
    "        self.stage4 = torch.nn.Sequential()\n",
    "        self.stage5 = torch.nn.Sequential()\n",
    "        for x in range(0,4):\n",
    "            self.stage1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        self.stage2.add_module(str(4), L2pooling(channels=64))\n",
    "        for x in range(5, 9):\n",
    "            self.stage2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        self.stage3.add_module(str(9), L2pooling(channels=128))\n",
    "        for x in range(10, 16):\n",
    "            self.stage3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        self.stage4.add_module(str(16), L2pooling(channels=256))\n",
    "        for x in range(17, 23):\n",
    "            self.stage4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        self.stage5.add_module(str(23), L2pooling(channels=512))\n",
    "        for x in range(24, 30):\n",
    "            self.stage5.add_module(str(x), vgg_pretrained_features[x])\n",
    "    \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1,-1,1,1))\n",
    "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1,-1,1,1))\n",
    "\n",
    "        self.chns = [3,64,128,256,512,512]\n",
    "        self.register_parameter(\"alpha\", nn.Parameter(torch.randn(1, sum(self.chns),1,1)))\n",
    "        self.register_parameter(\"beta\", nn.Parameter(torch.randn(1, sum(self.chns),1,1)))\n",
    "        self.alpha.data.normal_(0.1,0.01)\n",
    "        self.beta.data.normal_(0.1,0.01)\n",
    "        if load_weights:\n",
    "            weights = torch.load('notebooks/dists_weights.pt')\n",
    "            self.alpha.data = weights['alpha']\n",
    "            self.beta.data = weights['beta']\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        h = (x-self.mean)/self.std\n",
    "        h = self.stage1(h)\n",
    "        h_relu1_2 = h\n",
    "        h = self.stage2(h)\n",
    "        h_relu2_2 = h\n",
    "        h = self.stage3(h)\n",
    "        h_relu3_3 = h\n",
    "        h = self.stage4(h)\n",
    "        h_relu4_3 = h\n",
    "        h = self.stage5(h)\n",
    "        h_relu5_3 = h\n",
    "        return [x,h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3]\n",
    "\n",
    "    def forward(self, x, y, require_grad=False, batch_average=False):\n",
    "        if require_grad:\n",
    "            feats0 = self.forward_once(x)\n",
    "            feats1 = self.forward_once(y)   \n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                feats0 = self.forward_once(x)\n",
    "                feats1 = self.forward_once(y) \n",
    "        dist1 = 0 \n",
    "        dist2 = 0 \n",
    "        c1 = 1e-6\n",
    "        c2 = 1e-6\n",
    "\n",
    "        w_sum = self.alpha.sum() + self.beta.sum()\n",
    "        alpha = torch.split(self.alpha/w_sum, self.chns, dim=1)\n",
    "        beta = torch.split(self.beta/w_sum, self.chns, dim=1)\n",
    "        for k in range(len(self.chns)):\n",
    "            x_mean = feats0[k].mean([2,3], keepdim=True)\n",
    "            y_mean = feats1[k].mean([2,3], keepdim=True)\n",
    "\n",
    "            S1 = (2*x_mean*y_mean+c1)/(x_mean**2+y_mean**2+c1)\n",
    "\n",
    "            dist1 = dist1+(alpha[k]*S1).sum(1,keepdim=True)\n",
    "\n",
    "            x_var = ((feats0[k]-x_mean)**2).mean([2,3], keepdim=True)\n",
    "            y_var = ((feats1[k]-y_mean)**2).mean([2,3], keepdim=True)\n",
    "            xy_cov = (feats0[k]*feats1[k]).mean([2,3],keepdim=True) - x_mean*y_mean\n",
    "            S2 = (2*xy_cov+c2)/(x_var+y_var+c2)\n",
    "#             print(\"xy_cov\", xy_cov)\n",
    "#             print(\"x_var\", x_var)\n",
    "#             print(\"y_var\", y_var)\n",
    "#             print(\"S2\", S2)\n",
    "            dist2 = dist2+(beta[k]*S2).sum(1,keepdim=True)\n",
    "\n",
    "\n",
    "        score = 1 - (dist1+dist2).squeeze()\n",
    "        if batch_average:\n",
    "            return score.mean()\n",
    "        else:\n",
    "            return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-10T14:09:12.709306Z",
     "start_time": "2020-07-10T14:09:09.851775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.7881e-07, 1.7881e-07])\n"
     ]
    }
   ],
   "source": [
    "dists = DISTS(reduction='none')\n",
    "# dists = DISTS_orig()\n",
    "\n",
    "# prediction = torch.ones(2, 3, 32, 32)#.to(\"cuda\")\n",
    "# target = torch.ones(2, 3, 32,32)#.to(\"cuda\")\n",
    "# print(dists(prediction, target))\n",
    "\n",
    "prediction = torch.zeros(2, 3, 32,32)#.to(\"cuda\")\n",
    "target = torch.zeros(2, 3, 32, 32)#.to(\"cuda\")\n",
    "print(dists(prediction, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor([1.1921e-07, 1.1921e-07], grad_fn=<RsubBackward1>)\n",
    "tensor([1.1921e-07, 1.1921e-07], grad_fn=<RsubBackward1>)\n",
    "\n",
    "tensor(-0.3526)\n",
    "tensor(0.5778)\n",
    "\n",
    "EPS = 10e-6\n",
    "tensor(-0.0196, device='cuda:0')\n",
    "tensor(1.1921e-07, device='cuda:0')\n",
    "\n",
    "EPS = 10e-12\n",
    "tensor(-19586.9961, device='cuda:0')\n",
    "tensor(1.1921e-07, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:16:45.881646Z",
     "start_time": "2020-07-09T12:16:35.267376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2376, grad_fn=<RsubBackward1>) tensor(0.2377)\n",
      "tensor(0.3447, grad_fn=<RsubBackward1>) tensor(0.3447)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# My score\n",
    "dists = DISTS_orig()\n",
    "dists_my = DISTS_new()\n",
    "\n",
    "print(dists(I01, i1_01_5), dists_my(I01, i1_01_5))\n",
    "\n",
    "print(dists(goldhill_jpeg, goldhill), dists_my(goldhill_jpeg, goldhill))\n",
    "\n",
    "torch.isclose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:15:33.791280Z",
     "start_time": "2020-07-09T12:15:33.680843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 384, 512]) torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Read images\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "\n",
    "I01 = torch.tensor(imread('data/external/I01.BMP')).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "i1_01_5 = torch.tensor(imread('data/external/i01_01_5.bmp')).permute(2, 0, 1).unsqueeze(0) / 255.0\n",
    "\n",
    "\n",
    "goldhill = torch.tensor(imread('data/external/goldhill.gif')).unsqueeze(0).unsqueeze(0) / 255.0\n",
    "goldhill_jpeg = torch.tensor(imread('data/external/goldhill_jpeg.gif')).unsqueeze(0).unsqueeze(0) / 255.0\n",
    "\n",
    "\n",
    "print(I01.shape, goldhill.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-09T12:15:43.941066Z",
     "start_time": "2020-07-09T12:15:40.277650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3447, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original score\n",
    "dists_orig = DISTS_orig()\n",
    "dists_orig(goldhill_jpeg, goldhill)\n",
    "\n",
    "\n",
    "# prediction = torch.rand(3, 1, 96, 96)\n",
    "# target = torch.rand(3, 1, 96, 96)\n",
    "\n",
    "# dists_orig(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T17:08:17.717880Z",
     "start_time": "2020-07-07T17:08:17.688611Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read images\n",
    "import cv2\n",
    "from skimage.io import imread\n",
    "\n",
    "I01 = torch.tensor(imread('data/external/I01.BMP')).permute(2, 0, 1)\n",
    "i1_01_5 = torch.tensor(imread('data/external/i01_01_5.bmp')).permute(2, 0, 1)\n",
    "\n",
    "goldhill = torch.tensor(imread('data/external/goldhill.gif'))\n",
    "goldhill_jpeg = torch.tensor(imread('data/external/goldhill_jpeg.gif'))\n",
    "\n",
    "\n",
    "\n",
    "# print(\"i1_01_5 FSIM\", fsim(i1_01_5, I01, data_range=255, chromatic=False))\n",
    "# print(\"i1_01_5 FSIMc\", fsim(i1_01_5, I01, data_range=255, chromatic=True))\n",
    "\n",
    "# print(\"goldhill_jpeg FSIM\", fsim(goldhill_jpeg, goldhill, data_range=255, chromatic=False))\n",
    "\n",
    "# fsim = \n",
    "# ------------------\n",
    "# image1 = mpimg.imread('data/external/I01.BMP')\n",
    "# image2 = mpimg.imread('data/external/i01_01_5.bmp')\n",
    "\n",
    "# Convert to tensor and create fake batch\n",
    "# image1_t_cuda = torch.tensor(image1).permute(2, 0, 1).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).permute(2, 0, 1).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image3).permute(2, 0, 1).unsqueeze(0).cuda()\n",
    "# print(\"Input image shape:\", image1_t_cuda.shape)\n",
    "\n",
    "# image1_t_cuda.requires_grad_()\n",
    "# loss = fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1, chromatic=True)\n",
    "# loss.backward()\n",
    "\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).repeat(3, 1, 1, 1).cuda() / 255.\n",
    "\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "\n",
    "# fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1.0, chromatic=False)\n",
    "# image1_t_cuda = torch.tensor(image1).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "# image2_t_cuda = torch.tensor(image2).unsqueeze(0).unsqueeze(0).cuda() / 255.\n",
    "\n",
    "# image1_t_cuda = torch.rand(3, 3, 8, 8)\n",
    "# image2_t_cuda = torch.rand(3, 3, 8, 8)\n",
    "# fsim(image1_t_cuda, image2_t_cuda, reduction='none', data_range=1.0, chromatic=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:49:51.980128Z",
     "start_time": "2020-07-01T16:49:51.946061Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T21:56:39.910219Z",
     "start_time": "2020-07-01T21:56:39.907556Z"
    }
   },
   "outputs": [],
   "source": [
    "from piq import fsim, psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:21:27.833929Z",
     "start_time": "2020-07-01T22:21:27.798726Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:21:29.325862Z",
     "start_time": "2020-07-01T22:21:28.853518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1123, 2.9341, 2.9150, 3.0244])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.zeros(4, 3, 128, 128)\n",
    "# y = torch.zeros(4, 3, 128, 128)\n",
    "x = torch.rand(4, 3, 128, 128)\n",
    "y = torch.rand(4, 3, 128, 128)\n",
    "\n",
    "# x = torch.ones(4, 3, 128, 128)\n",
    "# y = torch.ones(4, 3, 128, 128)\n",
    "fsim(x, y, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T19:35:28.337801Z",
     "start_time": "2020-07-01T19:35:28.314338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test different Log Gabor filters\n",
    "\n",
    "size = (1024, 512)\n",
    "def get_meshgrid(size: Tuple[int, int]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        size: Shape of meshgrid to create\n",
    "    \"\"\"\n",
    "    if size[0] % 2:\n",
    "        # Odd\n",
    "        x = torch.range(-(size[0] - 1) / 2, (size[0] - 1) / 2) / (size[0] - 1)\n",
    "    else:\n",
    "        # Even\n",
    "        x = torch.range(- size[0] / 2, size[0] / 2 - 1) / size[0]\n",
    "    \n",
    "    if size[1] % 2:\n",
    "        # Odd\n",
    "        y = torch.range(-(size[1] - 1) / 2, (size[1] - 1) / 2) / (size[1] - 1)\n",
    "    else:\n",
    "        # Even\n",
    "        y = torch.range(- size[1] / 2, size[1] / 2 - 1) / size[1]\n",
    "    return torch.meshgrid(x, y)\n",
    "\n",
    "# # lp = _lowpassfilter(size=(H, W), cutoff=.45, n=15)\n",
    "# sigma_y = sigma/gamma\n",
    "# g0 = torch.exp(-0.5 * ((x1**2 / sigma**2) + (y1**2 / sigma_y**2)))\n",
    "        \n",
    "        \n",
    "def log_gabor_filter(size: Tuple[int, int], omega_0: float, sigma_f: float) -> torch.Tensor:\n",
    "    \"\"\"Constructs log Gabor filter of given shape. \n",
    "    \"\"\"\n",
    "    grid_x, grid_y = get_meshgrid(size)\n",
    "    \n",
    "    radius = torch.sqrt(grid_x ** 2 + grid_y ** 2)\n",
    "    radius = ifftshift(radius)\n",
    "    radius[0, 0] = 1\n",
    "    \n",
    "    gabor_filter = torch.exp((- torch.log(radius / omega_0) ** 2) / (2 * math.log(sigma_f) ** 2))\n",
    "    return gabor_filter\n",
    "\n",
    "\n",
    "def log_gabor_filter2(size: Tuple[int, int], omega_0: float = 0.021, sigma_f: float = 1.34) -> torch.Tensor:\n",
    "    \n",
    "    xx, yy = get_meshgrid(size)\n",
    "\n",
    "    mask = xx.pow(2) + yy.pow(2) <= 0.25\n",
    "    xx = xx * mask\n",
    "    yy = yy * mask\n",
    "\n",
    "    xx = ifftshift(xx)\n",
    "    yy = ifftshift(yy)\n",
    "\n",
    "    r = (xx.pow(2) + yy.pow(2)).sqrt()\n",
    "    r[0, 0] = 1\n",
    "    \n",
    "    lg = torch.exp((- torch.log(r / omega_0) ** 2) / (2 * math.log(sigma_f) ** 2))\n",
    "\n",
    "    lg[0, 0] = 0\n",
    "    return lg\n",
    "\n",
    "# lg = log_gabor_filter(size, omega_0=0.021, sigma_f=1.34)\n",
    "\n",
    "xx, yy = get_meshgrid(size)\n",
    "mask = xx.pow(2) + yy.pow(2) <= 0.25\n",
    "\n",
    "# lg = lg # * mask\n",
    "\n",
    "# lg2 = log_gabor_filter2(size, omega_0=0.021, sigma_f=1.34)\n",
    "\n",
    "# torch.sum(lg - lg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T19:35:29.521956Z",
     "start_time": "2020-07-01T19:35:29.410281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fec9cb38e80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T16:54:38.216609Z",
     "start_time": "2020-07-01T16:54:37.463371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i1_01_5 VSI tensor(0.9521)\n"
     ]
    }
   ],
   "source": [
    "print(\"i1_01_5 VSI\", vsi(I01, i1_01_5, data_range=255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T12:24:03.103357Z",
     "start_time": "2020-04-26T12:24:03.099630Z"
    }
   },
   "outputs": [],
   "source": [
    "# mnist = torchvision.datasets.MNIST(\"../datasets\", train=True, download=True)\n",
    "# cifar10 = torchvision.datasets.CIFAR10(\"../datasets\", download=True)\n",
    "# cifar100 = torchvision.datasets.CIFAR100(\"../datasets\", download=True)\n",
    "# fashin_mnist = torchvision.datasets.FashionMNIST(\"datasets\", download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T17:41:48.647750Z",
     "start_time": "2020-06-03T17:41:24.437192Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -P data/raw http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
    "# !wget -P data/raw http://vllab.ucmerced.edu/wlai24/LapSRN/results/SR_testing_datasets.zip\n",
    "# !wget -P data/raw http://www.cs.columbia.edu/CAVE/databases/SLAM_coil-20_coil-100/coil-100/coil-100.zip\n",
    "# !wget -P data/raw http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_LR_bicubic_X2.zip\n",
    "# !wget -P data/raw http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_LR_bicubic_X2.zip\n",
    "# !wget -P data/raw http://www.ponomarenko.info/tid2013/tid2013.rar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T11:49:48.118824Z",
     "start_time": "2020-06-25T11:49:33.826033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-06-25 14:49:33--  http://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip\n",
      "Resolving datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)... 134.34.224.175\n",
      "Connecting to datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)|134.34.224.175|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip [following]\n",
      "--2020-06-25 14:49:33--  https://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip\n",
      "Connecting to datasets.vqa.mmsp-kn.de (datasets.vqa.mmsp-kn.de)|134.34.224.175|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 766965996 (731M) [application/zip]\n",
      "Saving to: ‘datasets/koniq10k_512x384.zip’\n",
      "\n",
      "koniq10k_512x384.zi 100%[===================>] 731.44M  54.4MB/s    in 14s     \n",
      "\n",
      "2020-06-25 14:49:48 (53.3 MB/s) - ‘datasets/koniq10k_512x384.zip’ saved [766965996/766965996]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget -P data/raw http://datasets.vqa.mmsp-kn.de/archives/koniq10k_512x384.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-25T11:50:24.625456Z",
     "start_time": "2020-06-25T11:50:24.503496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdatasets\u001b[0m/  Development.ipynb  Tests.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TID2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-11T12:07:18.946020Z",
     "start_time": "2020-06-11T12:07:18.929999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'albu_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ad52812453c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0malbu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# to [0, 1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0malbu_pt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensorV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'albu_pt' is not defined"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import TID2013\n",
    "\n",
    "transform = albu.Compose([\n",
    "#     albu.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # to [-1, 1]\n",
    "    albu.Normalize(mean=[0., 0., 0.], std=[1., 1., 1.]), # to [0, 1]\n",
    "    albu_pt.ToTensorV2(),\n",
    "])\n",
    "\n",
    "dataset = TID2013(transform=transform)\n",
    "distorted_images, reference_images, scores = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:50:26.291463Z",
     "start_time": "2020-06-08T08:50:26.122445Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.augmentations import get_aug\n",
    "from src.utils import walk_files\n",
    "from src.datasets import *\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T08:53:13.529231Z",
     "start_time": "2020-06-08T08:50:31.586592Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c99e75bb6d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTASK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repoz/metrics-comparison/src/augmentations.py\u001b[0m in \u001b[0;36mget_aug\u001b[0;34m(aug_type, task, dataset, size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Add the same noise for all channels for single-channel images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMEAN_STD_BY_NAME\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"medicaldecathlon\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0msinglechannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "TASK = \"denoise\"\n",
    "SIZE = 256\n",
    "\n",
    "dataset_class = {\n",
    "    \"mnist\": MNIST,\n",
    "    \"fashion_mnist\": FashionMNIST, \n",
    "    \"cifar10\": CIFAR10,\n",
    "    \"cifar100\": CIFAR100,\n",
    "    \"tinyimagenet\": TinyImageNet,\n",
    "    \"div2k\": DIV2K,\n",
    "    \"set5\": Set5,\n",
    "    \"set14\": Set14,\n",
    "    \"urba100\": Urban100,\n",
    "    \"manga109\": Manga109,\n",
    "    \"coil100\": COIL100,\n",
    "    \"bsds100\": BSDS100,\n",
    "    \"medicaldecathlon\": MedicalDecathlon\n",
    "}\n",
    "\n",
    "\n",
    "dataset_names = [\n",
    "    \"div2k\",\n",
    "    \"bsds100\",\n",
    "#     \"set5\"\n",
    "]\n",
    "\n",
    "datasets = []\n",
    "for dataset_name in dataset_names:\n",
    "    transform = get_aug(aug_type=\"medium\", task=TASK, dataset=datasets, size=SIZE)\n",
    "    datasets.append(dataset_class[dataset_name](train=True, transform=transform))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    input, target = dataset[1]\n",
    "    print(\"Input\", input.shape, input.min(), input.max(), input.mean())\n",
    "    print(\"Target\", target.shape, target.min(), target.max(), target.mean())\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0) * 0.5 + 0.5, )\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T13:03:51.120720Z",
     "start_time": "2020-05-29T13:03:51.118706Z"
    }
   },
   "outputs": [],
   "source": [
    "# total_mean, total_var = [], []\n",
    "# for i in range(len(div2k)):\n",
    "#     image = div2k[i][1]\n",
    "#     total_mean.append(image.mean(dim=[1,2]))\n",
    "#     total_var.append(image.var(dim=[1,2]))\n",
    "# print(torch.stack(total_mean).mean(dim=0))\n",
    "# print(torch.stack(total_var).mean(dim=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T15:59:56.557719Z",
     "start_time": "2020-05-29T15:59:55.444859Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T12:28:43.539258Z",
     "start_time": "2020-05-29T12:28:28.385984Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"denoise\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T14:59:14.735977Z",
     "start_time": "2020-04-26T14:58:51.307570Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get dataset with no transform\n",
    "# AUG = get_aug(aug_type='light', dataset=\"medicaldecathlon\", task=\"deblur\", size=256)\n",
    "\n",
    "# medicaldecathlon = MedicalDecathlon(train=False, transform=AUG)\n",
    "# image = medicaldecathlon[135][0]\n",
    "# image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:00:22.313189Z",
     "start_time": "2020-04-26T15:00:22.300261Z"
    }
   },
   "outputs": [],
   "source": [
    "print(image.min(), image.max(), image.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:02.588910Z",
     "start_time": "2020-04-26T10:46:02.235819Z"
    }
   },
   "outputs": [],
   "source": [
    "# idx = 356\n",
    "print(image[..., 0].min(), image[..., 0].max(), image[...,0].mean())\n",
    "# plt.hist()\n",
    "# print(image.min(), image.max(), image.mean())\n",
    "# grey_image = image[..., 0]\n",
    "augmented = AUG(image=image, mask=image)\n",
    "input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "## Get gaussian\n",
    "# random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n",
    "# gauss = random_state.normal(0, 0.1, input.shape)\n",
    "# input = input + gauss\n",
    "# gauss\n",
    "\n",
    "# print(\"Input\", input.shape, input.min(), input.max())\n",
    "# print(\"Target\", target.shape, target.min(), target.max())\n",
    "\n",
    "# augmented = NORM_TO_TENSOR(image=input, mask=target)\n",
    "# input, target = augmented[\"image\"], augmented[\"mask\"]\n",
    "\n",
    "print(\"Input\", input.shape, input.min(), input.max())\n",
    "print(\"Target\", target.shape, target.min(), target.max())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(torch.cat([input[0], target[0]], dim=1)) #.permute(1, 2, 0))\n",
    "# plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0))\n",
    "\n",
    "torch.sum(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T10:46:15.660432Z",
     "start_time": "2020-04-26T10:46:15.622703Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.mean(input - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-25T10:12:01.132632Z",
     "start_time": "2020-04-25T10:12:01.115628Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow((medicaldecathlon[idx][0][..., 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:31:31.339877Z",
     "start_time": "2020-04-24T21:31:31.334100Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(256, 256)\n",
    "a = a[:,:,np.newaxis].repeat(3, axis=2)\n",
    "# a.repeat(3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T21:04:44.034846Z",
     "start_time": "2020-04-24T21:04:43.900165Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(torch.cat([input, target], dim=2).permute(1, 2, 0)[:, :, [2, 1, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:00:05.101620Z",
     "start_time": "2020-05-29T16:00:05.098635Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:18:18.394762Z",
     "start_time": "2020-05-29T16:18:10.979470Z"
    }
   },
   "outputs": [],
   "source": [
    "datasets = [\"div2k\", \"bsds100\", \"set5\"]\n",
    "transform = get_aug(aug_type=\"val\", task=\"deblur\", size=128)\n",
    "\n",
    "for dataset in datasets:\n",
    "#     train_loader = get_dataloader(dataset, transform, train=True, batch_size=64)\n",
    "    \n",
    "#     for batch in train_loader:\n",
    "#         input, output = batch\n",
    "#         print(input.shape, output.shape)\n",
    "        \n",
    "    \n",
    "    val_loader = get_dataloader(dataset, transform, train=False)\n",
    "    for batch in val_loader:\n",
    "        input, output = batch\n",
    "        print(input.shape, output.shape)\n",
    "    \n",
    "    \n",
    "    for batch in train_loader:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T16:14:07.954107Z",
     "start_time": "2020-05-29T16:14:07.942725Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = BSDS100(\"datasets/BSDS100\", train=True, transform=transform)\n",
    "dataset[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T18:52:16.029947Z",
     "start_time": "2020-04-14T18:52:16.024488Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "image_metrics = [\"kid\", {}, \"ssim\", {}, ]\n",
    "\n",
    "METRIC_FROM_NAME = {\n",
    "    \"ssim\" : pm.SSIMLoss,\n",
    "    \"ms-ssim\" : pm.MultiScaleSSIMLoss,\n",
    "    \"msid\" : pm.MSID,\n",
    "    \"fid\" : pm.FID,\n",
    "    \"kid\" : pm.KID,\n",
    "#     \"content\" : ContentLoss,\n",
    "#     \"style\" : StyleLoss,\n",
    "    \"tv\" : pm.TVLoss,\n",
    "}\n",
    "\n",
    "\n",
    "# for metric in image_metrics:\n",
    "image_metrics = [METRIC_FROM_NAME[metric](**kwargs) for metric, kwargs in zip(image_metrics[::2], image_metrics[1::2])]\n",
    "# list(zip(image_metrics[::2], image_metrics[1::2]))\n",
    "image_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T19:29:09.787795Z",
     "start_time": "2020-04-14T19:29:09.783997Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "output = OrderedDict({\n",
    "    'loss': 1,\n",
    "#     'mse': mse,\n",
    "#     'psnr': psnr,\n",
    "#     'ssim': ssim_score,\n",
    "#     # 'val_ms_ssim': ms_ssim_score,\n",
    "#     'input_features': input_features,\n",
    "#     'target_features': target_features\n",
    "})\n",
    "output[\"test\"] = 3\n",
    "output[\"foo\"] = \"bar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-26T15:08:23.644634Z",
     "start_time": "2020-04-26T15:08:23.508619Z"
    }
   },
   "outputs": [],
   "source": [
    "images = torch.rand((512, 2, 32, 32))\n",
    "target = torch.rand((512, 2, 64, 64))\n",
    "print(f\"Before interpolation: images {images.shape}\")\n",
    "images = F.interpolate(images, size=target.shape[-2:], mode=\"bilinear\")\n",
    "print(f\"After interpolation: images {images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import b\n",
    "# \n",
    "iter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T20:36:25.754041Z",
     "start_time": "2020-04-14T20:36:25.750728Z"
    }
   },
   "outputs": [],
   "source": [
    "a = [\"foo\", \"bar\"]\n",
    "if \"foo\" in a:\n",
    "    print(a.index(\"foo\"))\n",
    "# a.append(\"loss\")\n",
    "# print(a)\n",
    "# a.pop()\n",
    "# a\n",
    "# for i in \"etc\", a:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:36.482862Z",
     "start_time": "2020-04-14T16:45:34.767973Z"
    }
   },
   "outputs": [],
   "source": [
    "features = make_layers(\n",
    "    [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"], \n",
    "    batch_norm=True\n",
    ")\n",
    "vgg_16 = VGG(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:45:37.757692Z",
     "start_time": "2020-04-14T16:45:37.751504Z"
    }
   },
   "outputs": [],
   "source": [
    "vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:10.728420Z",
     "start_time": "2020-04-14T16:47:09.880830Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.rand(10, 3, 224, 224)\n",
    "layers = None\n",
    "if layers is None:\n",
    "    layers = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n",
    "\n",
    "features = []\n",
    "for name, module in vgg_16.features._modules.items():\n",
    "    x = module(x)\n",
    "    if name in layers:\n",
    "        features.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T16:47:17.479314Z",
     "start_time": "2020-04-14T16:47:17.414244Z"
    }
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:30:14.494629Z",
     "start_time": "2020-04-14T14:30:14.490716Z"
    }
   },
   "outputs": [],
   "source": [
    "# from albumentations import ImageOnlyTransform\n",
    "albu.ImageCompression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:20.318480Z",
     "start_time": "2020-04-14T13:26:20.314695Z"
    }
   },
   "outputs": [],
   "source": [
    "target.permute(2, 0, 1).shape, input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T13:26:30.322997Z",
     "start_time": "2020-04-14T13:26:30.167211Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(target.permute(2, 0, 1).transpose(0, 2).transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:18:02.700784Z",
     "start_time": "2020-04-14T11:18:02.475697Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = TinyImageNetDataset(train=True, transform=None, target_transform=None)\n",
    "input, target = dataset[6]\n",
    "# input.shape\n",
    "# plt.imshow(input / 255.)\n",
    "(input / 255.).min(), (input / 255.).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:09:29.108056Z",
     "start_time": "2020-04-14T11:09:29.104053Z"
    }
   },
   "outputs": [],
   "source": [
    "input.transpose(0, 2).transpose(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T11:02:42.360140Z",
     "start_time": "2020-04-14T11:02:42.355048Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-09T10:52:13.348128Z",
     "start_time": "2020-04-09T10:52:13.337949Z"
    }
   },
   "outputs": [],
   "source": [
    "# from src.augmentations import get_aug\n",
    "# from src.datasets import MNIST, CIFAR10, CIFAR100\n",
    "from ..src.datasets\n",
    "# from src.datasets import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:11.714091Z",
     "start_time": "2020-04-08T14:27:11.660050Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type=\"light\", size=32)\n",
    "target_transform = get_aug(aug_type=\"val\", size=32)\n",
    "# target_transform = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:12.035620Z",
     "start_time": "2020-04-08T14:27:11.981391Z"
    }
   },
   "outputs": [],
   "source": [
    "transform, target_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.384001Z",
     "start_time": "2020-04-08T14:27:12.625838Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = CIFAR10(root='../datasets', train=True, transform=transform, target_transform=target_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "# valset = CIFAR10(root='../datasets', train=False, transform=transform, target_transform=target_transform)\n",
    "# valloader = torch.utils.data.DataLoader(valset, batch_size=16,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:13.589831Z",
     "start_time": "2020-04-08T14:27:13.534077Z"
    }
   },
   "outputs": [],
   "source": [
    "image, target = trainset[0]\n",
    "print(image.shape, target.shape)\n",
    "print(image.max(), image.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:14.716283Z",
     "start_time": "2020-04-08T14:27:14.483971Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "input, target = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:27:16.860006Z",
     "start_time": "2020-04-08T14:27:16.789254Z"
    }
   },
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:04.837084Z",
     "start_time": "2020-04-08T15:09:04.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "model.fc = Identity()\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:07.391805Z",
     "start_time": "2020-04-08T15:09:07.348692Z"
    }
   },
   "outputs": [],
   "source": [
    "mock = torch.rand((3, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:09:08.273261Z",
     "start_time": "2020-04-08T15:09:08.185666Z"
    }
   },
   "outputs": [],
   "source": [
    "result = model(mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:15:02.858962Z",
     "start_time": "2020-04-08T15:15:02.810410Z"
    }
   },
   "outputs": [],
   "source": [
    "all_input_features = [result.detach() for _ in range(4)]\n",
    "print(len(all_input_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T15:16:17.105471Z",
     "start_time": "2020-04-08T15:16:17.056511Z"
    }
   },
   "outputs": [],
   "source": [
    "input_features = torch.cat(all_input_features, dim=0)\n",
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T14:54:58.027998Z",
     "start_time": "2020-04-08T14:54:57.979447Z"
    }
   },
   "outputs": [],
   "source": [
    "input.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:29:33.595967Z",
     "start_time": "2020-04-24T17:29:33.564780Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:11:48.608920Z",
     "start_time": "2020-04-24T19:11:48.604678Z"
    }
   },
   "outputs": [],
   "source": [
    "datapath = \"datasets/decathlon/colon.h5\"\n",
    "with h5py.File(datapath, \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "#     data_val = f['imgs_validation'][::10]\n",
    "#     data_test = f['imgs_testing'][::10]\n",
    "#     data = np.concatenate((data_val, data_test))\n",
    "#     print(data.shape)\n",
    "#     print(len(data))\n",
    "    \n",
    "\n",
    "# hf = \n",
    "# data_numpy = np.zeros(hf['imgs_train'].shape, dtype=numpy_type)\n",
    "# # hf['dataset_name'].read_direct(n1)\n",
    "# # hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:17:10.798726Z",
     "start_time": "2020-04-24T19:17:10.390446Z"
    }
   },
   "outputs": [],
   "source": [
    "data = hf['imgs_train'][5000]\n",
    "mask = hf['msks_train'][5000]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(data.squeeze(), cmap='gray')\n",
    "# plt.imshow(mask.squeeze(), alpha=0.1)\n",
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T19:13:31.176828Z",
     "start_time": "2020-04-24T19:13:30.998749Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:43:43.812272Z",
     "start_time": "2020-04-24T17:43:43.677479Z"
    }
   },
   "outputs": [],
   "source": [
    "# data.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T17:31:12.884247Z",
     "start_time": "2020-04-24T17:31:12.881134Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:06:57.805822Z",
     "start_time": "2020-05-13T16:06:57.802189Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3                                                                                                                 \n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import photosynthesis_metrics as pm    \n",
    "from photosynthesis_metrics import IS\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T16:07:03.862241Z",
     "start_time": "2020-05-13T16:07:03.179834Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "cifar10 = CIFAR10(root=\"/home/zakirov/repoz/metrics-comparison/datasets\", train=True, transform=aug)\n",
    "cifar10\n",
    "loader = torch.utils.data.DataLoader(cifar10, batch_size=100, num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T15:16:23.449739Z",
     "start_time": "2020-05-13T15:16:20.585333Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = pm.feature_extractors.fid_inception.InceptionV3(\n",
    "#     resize_input=True, \n",
    "#     normalize_input=True,\n",
    "#     requires_grad=False,\n",
    "#     use_fid_inception=False,\n",
    "# )\n",
    "model = inception_v3(pretrained=True, transform_input=False).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T20:43:51.665311Z",
     "start_time": "2020-05-19T20:43:51.663129Z"
    }
   },
   "outputs": [],
   "source": [
    "import photosynthesis_metrics as pm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-18T18:37:01.177723Z",
     "start_time": "2020-05-18T18:37:01.119361Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = pm.GS(num_iters=20)\n",
    "f1 = torch.rand(1000, 20)\n",
    "f2 = torch.rand(1000, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T21:22:15.649984Z",
     "start_time": "2020-05-19T21:22:15.644455Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read 2 images used in acticle and compute score for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:56:58.494183Z",
     "start_time": "2020-05-20T13:56:58.093684Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = get_aug(aug_type='light', task='deblur', size=64)\n",
    "\n",
    "loader = get_dataloader(\n",
    "    datasets=['tinyimagenet'],\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    batch_size=4\n",
    ")\n",
    "\n",
    "for batch in loader:\n",
    "    break\n",
    "    \n",
    "lr = batch[0][1]\n",
    "hr = batch[1][1]\n",
    "print(lr.min(), lr.max())\n",
    "plt.imshow((lr.permute(1,2,0) + 1) * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T22:26:16.320593Z",
     "start_time": "2020-07-01T22:26:16.307301Z"
    }
   },
   "outputs": [],
   "source": [
    "def fsim(x: torch.Tensor, y: torch.Tensor, reduction: str = 'mean',\n",
    "         data_range: Union[int, float] = 1.0, chromatic: bool = True,\n",
    "         scales: int = 4, orientations: int = 4, min_length: int = 6,\n",
    "         mult: int = 2, sigma_f: float = 0.55, delta_theta: float = 1.2,\n",
    "         k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Feature Similarity Index Measure for a batch of images.\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        x: Batch of predicted images with shape (batch_size x channels x H x W)\n",
    "        y: Batch of target images with shape  (batch_size x channels x H x W)\n",
    "        data_range: Value range of input images (usually 1.0 or 255). Default: 1.0\n",
    "        chromatic: Flag to compute FSIMc, which also takes into account chromatic components\n",
    "        scales: Number of wavelets used for computation of phase congruensy maps\n",
    "        orientations: Number of filter orientations used for computation of phase congruensy maps\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian describing the log Gabor filter's\n",
    "            transfer function in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations and the standard deviation\n",
    "            of the angular Gaussian function used to construct filters in the frequency plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean at which we set the noise\n",
    "            threshold  point, below which phase congruency values get penalized.\n",
    "        \n",
    "    Returns:\n",
    "        FSIM: Index of similarity betwen two images. Usually in [0, 1] interval.\n",
    "            Can be bigger than 1 for predicted images with higher contrast than original one.\n",
    "    Note:\n",
    "        This implementation is based on original authors MATLAB code.\n",
    "        https://www4.comp.polyu.edu.hk/~cslzhang/IQA/FSIM/FSIM.htm\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    _validate_input(input_tensors=(x, y), allow_5d=False)\n",
    "    x, y = _adjust_dimensions(input_tensors=(x, y))\n",
    "    \n",
    "    # Scale to [0., 1.] range\n",
    "    x = x / float(data_range)\n",
    "    y = y / float(data_range)\n",
    "    \n",
    "    # Rescale to [0, 255] range, because all constant are calculated for this factor\n",
    "    x = x * 255\n",
    "    y = y * 255\n",
    "    \n",
    "    # Apply average pooling\n",
    "    kernel_size = max(1, round(min(x.shape[-2:]) / 256))\n",
    "    x = torch.nn.functional.avg_pool2d(x, kernel_size, stride=2)\n",
    "    y = torch.nn.functional.avg_pool2d(y, kernel_size, stride=2)\n",
    "        \n",
    "    num_channels = x.size(1)\n",
    "\n",
    "    # Convert RGB to YIQ color space https://en.wikipedia.org/wiki/YIQ\n",
    "    if num_channels == 3:\n",
    "        yiq_weights = torch.tensor([\n",
    "            [0.299, 0.587, 0.114],\n",
    "            [0.5959, -0.2746, -0.3213],\n",
    "            [0.2115, -0.5227, 0.3112]]).t().to(x)\n",
    "        x = torch.matmul(x.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        y = torch.matmul(y.permute(0, 2, 3, 1), yiq_weights).permute(0, 3, 1, 2)\n",
    "        \n",
    "        x_lum = x[:, : 1, :, :]\n",
    "        y_lum = y[:, : 1, :, :]\n",
    "        \n",
    "        x_i = x[:, 1, :, :]\n",
    "        y_i = y[:, 1, :, :]\n",
    "        x_q = x[:, 2, :, :]\n",
    "        y_q = y[:, 2, :, :]\n",
    "\n",
    "    else:\n",
    "        x_lum = x\n",
    "        y_lum = y\n",
    "\n",
    "    # Compute phase congruency maps\n",
    "    pc_x = _phase_congruency(\n",
    "        x_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    pc_y = _phase_congruency(\n",
    "        y_lum, scales=scales, orientations=orientations,\n",
    "        min_length=min_length, mult=mult, sigma_f=sigma_f,\n",
    "        delta_theta=delta_theta, k=k\n",
    "    )\n",
    "    \n",
    "    # Gradient maps\n",
    "    kernels = torch.stack([scharr_filter(), scharr_filter().transpose(-1, -2)])\n",
    "    grad_map_x = gradient_map(x_lum, kernels)\n",
    "    grad_map_y = gradient_map(y_lum, kernels)\n",
    "    \n",
    "    # Constants from paper\n",
    "    T1, T2, T3, T4, lmbda = 0.85, 160, 200, 200, 0.03\n",
    "    \n",
    "    # Compute FSIM\n",
    "    PC = similarity_map(pc_x, pc_y, T1)\n",
    "    GM = similarity_map(grad_map_x, grad_map_y, T2)\n",
    "    pc_max = torch.where(pc_x > pc_y, pc_x, pc_y)\n",
    "    score = GM * PC * pc_max\n",
    "    \n",
    "    if chromatic:\n",
    "        S_I = similarity_map(x_i, y_i, T3)\n",
    "        S_Q = similarity_map(x_q, y_q, T4)\n",
    "        score = score * torch.abs(S_I * S_Q) ** lmbda\n",
    "        # Complex gradients will work in PyTorch 1.6.0\n",
    "        # score = score * torch.real((S_I * S_Q).to(torch.complex64) ** lmbda)\n",
    "\n",
    "    result = score.sum(dim=[1, 2, 3]) / pc_max.sum(dim=[1, 2])\n",
    "    \n",
    "    if reduction == 'none':\n",
    "        return result\n",
    "\n",
    "    return {'mean': result.mean,\n",
    "            'sum': result.sum\n",
    "            }[reduction](dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _phase_congruency(x: torch.Tensor, scales: int = 4, orientations: int = 4,\n",
    "                      min_length: int = 6, mult: int = 2, sigma_f: float = 0.55,\n",
    "                      delta_theta: float = 1.2, k: float = 2.0) -> torch.Tensor:\n",
    "    r\"\"\"Compute Phase Congruence for a batch of greyscale images\n",
    "\n",
    "    Args:\n",
    "        x: Tensor with shape Bx1xHxW\n",
    "        levels: Number of wavelet scales\n",
    "        orientations: Number of filter orientations\n",
    "        min_length: Wavelength of smallest scale filter\n",
    "        mult: Scaling factor between successive filters\n",
    "        sigma_f: Ratio of the standard deviation of the Gaussian\n",
    "            describing the log Gabor filter's transfer function\n",
    "            in the frequency domain to the filter center frequency.\n",
    "        delta_theta: Ratio of angular interval between filter orientations\n",
    "            and the standard deviation of the angular Gaussian function\n",
    "            used to construct filters in the freq. plane.\n",
    "        k: No of standard deviations of the noise energy beyond the mean\n",
    "            at which we set the noise threshold point, below which phase\n",
    "            congruency values get penalized.\n",
    "    Returns:\n",
    "        PCmap: Tensor with shape BxHxW\n",
    "\n",
    "    \"\"\"\n",
    "    EPS = 1e-4\n",
    "\n",
    "    B, _, H, W = x.shape\n",
    "\n",
    "    # Fourier transform\n",
    "    imagefft = torch.rfft(x, 2, onesided=False)\n",
    "\n",
    "    filters = _construct_filters(x, scales, orientations, min_length, mult, sigma_f, delta_theta, k)\n",
    "\n",
    "    # Note rescaling to match power record ifft2 of filter\n",
    "    filters_ifft = torch.ifft(torch.stack([filters, torch.zeros_like(filters)], dim=-1), 2)[..., 0] * math.sqrt(H * W)\n",
    "    \n",
    "    # Convolve image with even and odd filters\n",
    "    E0 = torch.ifft(imagefft * filters.unsqueeze(-1), 2).view(B, orientations, scales, H, W, 2)\n",
    "\n",
    "    # Amplitude of even & odd filter response. An = sqrt(real^2 + imag^2)\n",
    "    an = torch.sqrt(torch.sum(E0 ** 2, dim=-1))\n",
    "\n",
    "    # Take filter at scale 0 and sum spatially\n",
    "    # Record mean squared filter value at smallest scale.\n",
    "    # This is used for noise estimation.\n",
    "    em_n = (filters.view(1, orientations, scales, H, W)[:, :, :1, ...] ** 2).sum(dim=[-2, -1], keepdims=True)\n",
    "\n",
    "    # Sum of even filter convolution results.\n",
    "    sum_e = E0[..., 0].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Sum of odd filter convolution results.\n",
    "    sum_o = E0[..., 1].sum(dim=2, keepdims=True)\n",
    "    \n",
    "    # Get weighted mean filter response vector, this gives the weighted mean phase angle.\n",
    "    x_energy = torch.sqrt(sum_e ** 2 + sum_o ** 2) + EPS\n",
    "\n",
    "    mean_e = sum_e / x_energy\n",
    "    mean_o = sum_o / x_energy\n",
    "\n",
    "    # Now calculate An(cos(phase_deviation) - | sin(phase_deviation)) | by\n",
    "    # using dot and cross products between the weighted mean filter response\n",
    "    # vector and the individual filter response vectors at each scale.\n",
    "    # This quantity is phase congruency multiplied by An, which we call energy.\n",
    "\n",
    "    # Extract even and odd convolution results.\n",
    "    E = E0[..., 0]\n",
    "    O = E0[..., 1]\n",
    "\n",
    "    energy = (E * mean_e + O * mean_o - torch.abs(E * mean_o - O * mean_e)).sum(dim=2, keepdim=True)\n",
    "    \n",
    "    # Compensate for noise\n",
    "    # We estimate the noise power from the energy squared response at the\n",
    "    # smallest scale.  If the noise is Gaussian the energy squared will have a\n",
    "    # Chi-squared 2DOF pdf.  We calculate the median energy squared response\n",
    "    # as this is a robust statistic.  From this we estimate the mean.\n",
    "    # The estimate of noise power is obtained by dividing the mean squared\n",
    "    # energy value by the mean squared filter value\n",
    "    \n",
    "    abs_e0 = torch.sqrt(torch.sum(E0[:, :, :1, ...] ** 2, dim=-1)).reshape(B, orientations, 1, 1, H * W)\n",
    "    median_e2n = torch.median(abs_e0 ** 2, dim=-1, keepdims=True).values\n",
    "\n",
    "    mean_e2n = - median_e2n / math.log(0.5)\n",
    "\n",
    "    # Estimate of noise power.\n",
    "    noisePower = mean_e2n / em_n\n",
    "    \n",
    "    # Now estimate the total energy^2 due to noise\n",
    "    # Estimate for sum(An^2) + sum(Ai.*Aj.*(cphi.*cphj + sphi.*sphj))\n",
    "    filters_ifft = filters_ifft.view(1, orientations, scales, H, W)\n",
    "    \n",
    "    sum_an2 = torch.sum(filters_ifft ** 2, dim=-3, keepdim=True)\n",
    "    \n",
    "    sum_ai_aj = torch.zeros(B, orientations, 1, H, W).to(x)\n",
    "    for s in range(scales - 1):\n",
    "        sum_ai_aj = sum_ai_aj + (filters_ifft[:, :, s: s + 1] * filters_ifft[:, :, s + 1:]).sum(dim=-3, keepdim=True)\n",
    "            \n",
    "    sum_an2 = torch.sum(sum_an2, dim=[-1, -2], keepdim=True)\n",
    "    sum_ai_aj = torch.sum(sum_ai_aj, dim=[-1, -2], keepdim=True)\n",
    "\n",
    "    noise_energy2 = 2 * noisePower * sum_an2 + 4 * noisePower * sum_ai_aj\n",
    "\n",
    "    # Rayleigh parameter\n",
    "    tau = torch.sqrt(noise_energy2 / 2)\n",
    "\n",
    "    # Expected value of noise energy\n",
    "    noise_energy = tau * math.sqrt(math.pi / 2)\n",
    "    moise_energy_sigma = torch.sqrt((2 - math.pi / 2) * tau ** 2)\n",
    "\n",
    "    # Noise threshold\n",
    "    T = noise_energy + k * moise_energy_sigma\n",
    "\n",
    "    # The estimated noise effect calculated above is only valid for the PC_1 measure.\n",
    "    # The PC_2 measure does not lend itself readily to the same analysis.  However\n",
    "    # empirically it seems that the noise effect is overestimated roughly by a factor\n",
    "    # of 1.7 for the filter parameters used here.\n",
    "\n",
    "    # Empirical rescaling of the estimated noise effect to suit the PC_2 phase congruency measure\n",
    "    T = T / 1.7\n",
    "\n",
    "    # Apply noise threshold\n",
    "    energy = torch.max(energy - T, torch.zeros_like(T))\n",
    "\n",
    "    energy_all = energy.sum(dim=[1, 2])\n",
    "    \n",
    "    sum_an = an.sum(dim=2, keepdims=True)\n",
    "    an_all = sum_an.sum(dim=[1, 2])\n",
    "    # an_all = an.sum(dim=[1, 2])\n",
    "    \n",
    "    result_pc = energy_all / an_all\n",
    "    return result_pc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
